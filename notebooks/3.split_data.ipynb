{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. split data function\n",
    "def split_data_by_group(data, group_col, train_ratio=0.8, val_ratio=0.2, test_ratio=None):\n",
    "    \"\"\"\n",
    "    Splits the data based on 'namhoc' column and then by group.\n",
    "    - Data with namhoc <= 2020: split into train (80%) and validation (20%)\n",
    "    - Data with namhoc > 2020: assigned to test set\n",
    "\n",
    "    Parameters:\n",
    "    - data (pd.DataFrame): The dataset to split\n",
    "    - group_col (str): The column name to group by\n",
    "    - train_ratio (float): Proportion of historical data for training (default 0.8)\n",
    "    - val_ratio (float): Proportion of historical data for validation (default 0.2)\n",
    "    - test_ratio: Not used, kept for compatibility\n",
    "\n",
    "    Returns:\n",
    "    - train_set (pd.DataFrame): Training set\n",
    "    - val_set (pd.DataFrame): Validation set\n",
    "    - test_set (pd.DataFrame): Testing set\n",
    "    \"\"\"\n",
    "    assert 'namhoc' in data.columns, \"'namhoc' column must exist in the dataset\"\n",
    "    assert abs(train_ratio + val_ratio - 1.0) < 1e-5, \"Train and validation ratios must sum to 1\"\n",
    "    # List of columns to drops\n",
    "    drop_cols = ['Unnamed_0', 'diem_qt', 'diem_th', 'diem_gk', 'diem_ck']\n",
    "    # Drop unnecessary columns\n",
    "    data = data.drop(columns=drop_cols, errors='ignore')\n",
    "    # Validate namhoc values\n",
    "    print(f\"\\nnamhoc value counts:\\n{data['namhoc'].value_counts().sort_index()}\")\n",
    "    if data['namhoc'].isna().any():\n",
    "        print(\"Warning: Found NaN values in namhoc column. Filling with 0...\")\n",
    "        data['namhoc'] = data['namhoc'].fillna(0)\n",
    "\n",
    "    # First split: separate historical and future data\n",
    "    historical_data = data[data['namhoc'] <= 2020].copy()\n",
    "    test_set = data[data['namhoc'] > 2020].copy()\n",
    "    \n",
    "    print(f\"\\nInitial split sizes:\")\n",
    "    print(f\"Historical data (<=2020): {len(historical_data)} samples\")\n",
    "    print(f\"Future data (>2020): {len(test_set)} samples\")\n",
    "    \n",
    "    if len(historical_data) == 0:\n",
    "        print(\"Warning: No historical data found. Using 70-30 split on all data.\")\n",
    "        historical_data = data.copy()\n",
    "        test_set = pd.DataFrame()\n",
    "\n",
    "    train_set = pd.DataFrame()\n",
    "    val_set = pd.DataFrame()\n",
    "\n",
    "    # Process historical data by groups\n",
    "    grouped = historical_data.groupby(group_col)\n",
    "    print(f\"\\nNumber of groups: {len(grouped)}\")\n",
    "\n",
    "    for group, group_data in grouped:\n",
    "        n_samples = len(group_data)\n",
    "        if n_samples < 2:\n",
    "            train_set = pd.concat([train_set, group_data], ignore_index=True)\n",
    "            print(f\"Group '{group}' has only {n_samples} sample(s). Assigned to training set.\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Split into train and validation\n",
    "            train, val = train_test_split(\n",
    "                group_data, \n",
    "                test_size=val_ratio,\n",
    "                random_state=42,\n",
    "                shuffle=True\n",
    "            )\n",
    "\n",
    "            train_set = pd.concat([train_set, train], ignore_index=True)\n",
    "            val_set = pd.concat([val_set, val], ignore_index=True)\n",
    "            print(f\"Group '{group}' split into {len(train)} train and {len(val)} validation samples.\")\n",
    "        except ValueError as e:\n",
    "            print(f\"Error splitting group '{group}': {e}. Assigning all to training set.\")\n",
    "            train_set = pd.concat([train_set, group_data], ignore_index=True)\n",
    "\n",
    "    # Validate final sets\n",
    "    if len(train_set) == 0:\n",
    "        print(\"Warning: Empty training set. Using 80% of all data for training.\")\n",
    "        train_set = data.sample(frac=0.8, random_state=42)\n",
    "        val_set = data.drop(train_set.index)\n",
    "        test_set = pd.DataFrame()\n",
    "\n",
    "    print(\"\\nFinal split summary:\")\n",
    "    print(f\"Training set: {len(train_set)} samples\")\n",
    "    print(f\"Validation set: {len(val_set)} samples\")\n",
    "    print(f\"Test set: {len(test_set)} samples\")\n",
    "    \n",
    "    if len(train_set) == 0 or len(test_set) == 0:\n",
    "        print(\"\\nWarning: One or more sets are empty!\")\n",
    "        print(f\"Training set columns: {train_set.columns.tolist()}\")\n",
    "        print(f\"Test set columns: {test_set.columns.tolist()}\")\n",
    "\n",
    "    return train_set, val_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Define the column cleaning functions\n",
    "def clean_column_names(df):\n",
    "    \"\"\"\n",
    "    Clean column names by:\n",
    "    - Replacing non-alphanumeric characters with underscores.\n",
    "    - Ensuring column names start with a letter.\n",
    "    - Making column names unique.\n",
    "    \"\"\"\n",
    "    # Replace any sequence of non-word characters with a single underscore\n",
    "    df.columns = [\n",
    "        re.sub(r'\\W+', '_', col).strip('_') for col in df.columns\n",
    "    ]\n",
    "\n",
    "    # Ensure column names start with a letter by prefixing with 'f_' if necessary\n",
    "    df.columns = [\n",
    "        col if re.match(r'^[A-Za-z]', col) else f'f_{col}' for col in df.columns\n",
    "    ]\n",
    "\n",
    "    # Ensure uniqueness by appending suffixes to duplicate names\n",
    "    seen = {}\n",
    "    new_columns = []\n",
    "    for col in df.columns:\n",
    "        if col in seen:\n",
    "            seen[col] += 1\n",
    "            new_columns.append(f\"{col}_{seen[col]}\")\n",
    "        else:\n",
    "            seen[col] = 0\n",
    "            new_columns.append(col)\n",
    "    df.columns = new_columns\n",
    "\n",
    "    return df\n",
    "\n",
    "def verify_column_names(df):\n",
    "    \"\"\"\n",
    "    Verify that all column names consist of only alphanumeric characters and underscores,\n",
    "    and start with a letter.\n",
    "    \"\"\"\n",
    "    problematic_cols = [\n",
    "        col for col in df.columns\n",
    "        if not re.match(r'^[A-Za-z]\\w*$', col)\n",
    "    ]\n",
    "    return problematic_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset 1 shape: (354290, 71)\n",
      "All column names are clean and compatible with models.\n",
      "\n",
      "Dataset 5 shape: (36751, 71)\n",
      "All column names are clean and compatible with models.\n",
      "\n",
      "namhoc value counts:\n",
      "namhoc\n",
      "2013.0    13056\n",
      "2014.0    24521\n",
      "2015.0    34429\n",
      "2016.0    41593\n",
      "2017.0    45974\n",
      "2018.0    51925\n",
      "2019.0    59435\n",
      "2020.0    44194\n",
      "2021.0    29837\n",
      "2022.0     9326\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Initial split sizes:\n",
      "Historical data (<=2020): 315127 samples\n",
      "Future data (>2020): 39163 samples\n",
      "\n",
      "Number of groups: 22\n",
      "Group '0' split into 1 train and 1 validation samples.\n",
      "Group '1' split into 45041 train and 11261 validation samples.\n",
      "Group '2' split into 41328 train and 10332 validation samples.\n",
      "Group '3' split into 33400 train and 8351 validation samples.\n",
      "Group '4' split into 35388 train and 8848 validation samples.\n",
      "Group '5' split into 23596 train and 5899 validation samples.\n",
      "Group '6' split into 22887 train and 5722 validation samples.\n",
      "Group '7' split into 17988 train and 4498 validation samples.\n",
      "Group '8' split into 13372 train and 3343 validation samples.\n",
      "Group '9' split into 8288 train and 2072 validation samples.\n",
      "Group '10' split into 4795 train and 1199 validation samples.\n",
      "Group '11' split into 2572 train and 644 validation samples.\n",
      "Group '12' split into 1441 train and 361 validation samples.\n",
      "Group '13' split into 899 train and 225 validation samples.\n",
      "Group '14' split into 461 train and 116 validation samples.\n",
      "Group '15' split into 276 train and 69 validation samples.\n",
      "Group '16' split into 176 train and 44 validation samples.\n",
      "Group '17' split into 88 train and 23 validation samples.\n",
      "Group '18' split into 52 train and 13 validation samples.\n",
      "Group '19' split into 24 train and 6 validation samples.\n",
      "Group '20' split into 17 train and 5 validation samples.\n",
      "Group '21' split into 4 train and 1 validation samples.\n",
      "\n",
      "Final split summary:\n",
      "Training set: 252094 samples\n",
      "Validation set: 63033 samples\n",
      "Test set: 39163 samples\n",
      "\n",
      "namhoc value counts:\n",
      "namhoc\n",
      "2013.0     306\n",
      "2014.0     422\n",
      "2015.0    1382\n",
      "2016.0    2117\n",
      "2017.0    3508\n",
      "2018.0    4884\n",
      "2019.0    9364\n",
      "2020.0    7991\n",
      "2021.0    5061\n",
      "2022.0    1716\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Initial split sizes:\n",
      "Historical data (<=2020): 29974 samples\n",
      "Future data (>2020): 6777 samples\n",
      "\n",
      "Number of groups: 9\n",
      "Group '-1.3765396860558137' split into 5019 train and 1255 validation samples.\n",
      "Group '-0.9011218681605292' split into 3984 train and 997 validation samples.\n",
      "Group '-0.4257040502652449' split into 4792 train and 1198 validation samples.\n",
      "Group '0.0497137676300394' split into 4210 train and 1053 validation samples.\n",
      "Group '0.5251315855253239' split into 1925 train and 482 validation samples.\n",
      "Group '1.0005494034206082' split into 2192 train and 549 validation samples.\n",
      "Group '1.4759672213158928' split into 1281 train and 321 validation samples.\n",
      "Group '1.951385039211177' split into 508 train and 128 validation samples.\n",
      "Group '2.4268028571064613' split into 64 train and 16 validation samples.\n",
      "\n",
      "Final split summary:\n",
      "Training set: 23975 samples\n",
      "Validation set: 5999 samples\n",
      "Test set: 6777 samples\n",
      "Data split and saved successfully\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "data1 = pd.read_csv(\"/home/dev/project/modelling/preprocessing/dataset1.csv\")\n",
    "print(f\"\\nDataset 1 shape: {data1.shape}\")\n",
    "# Clean column names\n",
    "data1 = clean_column_names(data1)\n",
    "# Verify column names\n",
    "problematic_columns = verify_column_names(data1)\n",
    "if problematic_columns:\n",
    "    print(f\"Problematic columns after cleaning: {problematic_columns}\")\n",
    "    print(\"Further cleaning or renaming may be required.\")\n",
    "else:\n",
    "    print(\"All column names are clean and compatible with models.\")\n",
    "data5 = pd.read_csv(\"/home/dev/project/modelling/preprocessing/dataset5.csv\") \n",
    "print(f\"\\nDataset 5 shape: {data5.shape}\")\n",
    "# Clean column names\n",
    "data5 = clean_column_names(data5)\n",
    "# Verify column names\n",
    "problematic_columns = verify_column_names(data1)\n",
    "if problematic_columns:\n",
    "    print(f\"Problematic columns after cleaning: {problematic_columns}\")\n",
    "    print(\"Further cleaning or renaming may be required.\")\n",
    "else:\n",
    "    print(\"All column names are clean and compatible with models.\")\n",
    "# Define results directory\n",
    "results_dir = \"/home/dev/project/modelling/preprocessing/results\"\n",
    "\n",
    "# Create results directories for each dataset\n",
    "dataset1_dir = os.path.join(results_dir, \"dataset1\")\n",
    "dataset5_dir = os.path.join(results_dir, \"dataset5\")\n",
    "os.makedirs(dataset1_dir, exist_ok=True)\n",
    "os.makedirs(dataset5_dir, exist_ok=True)\n",
    "\n",
    "# Split and save dataset1\n",
    "train1, val1, test1 = split_data_by_group(data1, 'hocky_monhoc_count')\n",
    "train1.to_csv(os.path.join(dataset1_dir, \"train.csv\"), index=False)\n",
    "val1.to_csv(os.path.join(dataset1_dir, \"val.csv\"), index=False) \n",
    "test1.to_csv(os.path.join(dataset1_dir, \"test.csv\"), index=False)\n",
    "\n",
    "# Split and save dataset5\n",
    "train5, val5, test5 = split_data_by_group(data5, 'hocky_monhoc_count')\n",
    "train5.to_csv(os.path.join(dataset5_dir, \"train.csv\"), index=False)\n",
    "val5.to_csv(os.path.join(dataset5_dir, \"val.csv\"), index=False)\n",
    "test5.to_csv(os.path.join(dataset5_dir, \"test.csv\"), index=False)\n",
    "\n",
    "print(\"Data split and saved successfully\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
