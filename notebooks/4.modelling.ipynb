{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import threading\n",
    "import time\n",
    "import pickle\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "from lightgbm import LGBMRegressor\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# Import BayesSearchCV from scikit-optimize\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer, Categorical  # Added import\n",
    "\n",
    "def train_model(model_name, model, X_train, y_train, X_test, y_test, results, training_threshold, dataset_name, is_optimized=False, X_val=None, y_val=None):\n",
    "    y_pred = [None]\n",
    "    training_time = [None]\n",
    "    optimization_time = [0]\n",
    "    training_completed = [False]\n",
    "\n",
    "    def train():\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            model_type = \"optimized\" if is_optimized else \"default\"\n",
    "            print(f\"Starting training for {model_name} ({model_type} parameters)...\")\n",
    "            \n",
    "            if isinstance(model, BayesSearchCV) and is_optimized:\n",
    "                opt_start = time.time()\n",
    "                model.fit(X_train, y_train)\n",
    "                optimization_time[0] = time.time() - opt_start\n",
    "            else:\n",
    "                if 'LightGBM' in model_name and not is_optimized:\n",
    "                    # For default parameters, get the LightGBM estimator directly from pipeline\n",
    "                    lgb_estimator = model.named_steps['lightgbm']\n",
    "                    if X_val is not None and y_val is not None:\n",
    "                        lgb_estimator.fit(\n",
    "                            X_train, y_train,\n",
    "                            eval_set=[(X_val, y_val)],\n",
    "                            early_stopping_rounds=20\n",
    "                        )\n",
    "                    else:\n",
    "                        lgb_estimator.fit(X_train, y_train)\n",
    "                else:\n",
    "                    model.fit(X_train, y_train)\n",
    "            \n",
    "            y_pred[0] = model.predict(X_test)\n",
    "            training_time[0] = time.time() - start_time\n",
    "            training_completed[0] = True\n",
    "            print(f\"Completed training for {model_name} in {training_time[0]:.2f} seconds.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error training model {model_name}: {e}\")\n",
    "            training_completed[0] = False\n",
    "\n",
    "    thread = threading.Thread(target=train)\n",
    "    thread.start()\n",
    "    thread.join(timeout=training_threshold)\n",
    "\n",
    "    if not training_completed[0]:\n",
    "        print(f\"Model {model_name} exceeded training time ({training_threshold} seconds) or encountered an error.\")\n",
    "        y_pred[0] = np.nan\n",
    "        training_time[0] = np.nan\n",
    "    else:\n",
    "        mse = mean_squared_error(y_test, y_pred[0])\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(y_test, y_pred[0])\n",
    "        r_squared = r2_score(y_test, y_pred[0])\n",
    "\n",
    "        n = len(y_test)\n",
    "        p = X_test.shape[1]\n",
    "        if n > p + 1 and p > 0:\n",
    "            adjusted_r_squared = 1 - (1 - r_squared) * ((n - 1) / (n - p - 1))\n",
    "        else:\n",
    "            adjusted_r_squared = r_squared\n",
    "\n",
    "        print(f\"Model {model_name} trained successfully in {training_time[0]:.2f} seconds.\")\n",
    "        print(f\"MSE: {mse}, RMSE: {rmse}, MAE: {mae}, R²: {r_squared}, Adjusted R²: {adjusted_r_squared}\")\n",
    "\n",
    "        result = {\n",
    "            'Model': f\"{model_name} ({'Optimized' if is_optimized else 'Default'})\",\n",
    "            'Dataset': dataset_name,\n",
    "            'Parameter Type': 'Optimized' if is_optimized else 'Default',\n",
    "            'Training Time (s)': training_time[0],\n",
    "            'Optimization Time (s)': optimization_time[0],\n",
    "            'MSE': mse,\n",
    "            'RMSE': rmse,\n",
    "            'MAE': mae,\n",
    "            'R2 Score': r_squared,\n",
    "            'Adjusted R2 Score': adjusted_r_squared\n",
    "        }\n",
    "\n",
    "        if isinstance(model, BayesSearchCV) and is_optimized:\n",
    "            result['Best Params'] = str(model.best_params_)\n",
    "        elif not is_optimized:\n",
    "            result['Parameters'] = str(model.get_params())\n",
    "\n",
    "        results.append(result)\n",
    "\n",
    "def main():\n",
    "    # Define default parameters for each model\n",
    "    default_params = {\n",
    "        'Ridge Regression': {\n",
    "            'ridge__alpha': 1.0\n",
    "        },\n",
    "        'Lasso Regression': {\n",
    "            'lasso__alpha': 0.1\n",
    "        },\n",
    "        'Elastic Net Regression': {\n",
    "            'elasticnet__alpha': 0.1,\n",
    "            'elasticnet__l1_ratio': 0.5\n",
    "        },\n",
    "        'Random Forest Regression': {\n",
    "            'randomforest__n_estimators': 100,\n",
    "            'randomforest__max_depth': 10,\n",
    "            'randomforest__min_samples_split': 2\n",
    "        },\n",
    "        'XGBoost Regression': {\n",
    "            'xgboost__learning_rate': 0.01,\n",
    "            'xgboost__max_depth': 3,\n",
    "            'xgboost__n_estimators': 100\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Update parameter search spaces using skopt.space dimensions\n",
    "    param_spaces = {\n",
    "        'Ridge Regression': {\n",
    "            'ridge__alpha': Real(0.1, 5.0, prior='log-uniform')\n",
    "        },\n",
    "        'Lasso Regression': {\n",
    "            'lasso__alpha': Real(0.01, 0.5, prior='log-uniform')\n",
    "        },\n",
    "        'Elastic Net Regression': {\n",
    "            'elasticnet__alpha': Real(0.01, 0.5, prior='log-uniform'),\n",
    "            'elasticnet__l1_ratio': Real(0.2, 0.8)\n",
    "        },\n",
    "        'LightGBM Regression': {\n",
    "            'lightgbm__num_leaves': Integer(20, 40),\n",
    "            'lightgbm__learning_rate': Real(0.01, 0.03, prior='log-uniform'),\n",
    "            'lightgbm__n_estimators': Integer(50, 150)\n",
    "        },\n",
    "        'Random Forest Regression': {\n",
    "            'randomforest__n_estimators': Integer(50, 100),\n",
    "            'randomforest__max_depth': Categorical([5, 10]),\n",
    "            'randomforest__min_samples_split': Integer(2, 4)\n",
    "        },\n",
    "        'XGBoost Regression': {\n",
    "            'xgboost__learning_rate': Real(0.01, 0.03, prior='log-uniform'),\n",
    "            'xgboost__max_depth': Integer(3, 5),\n",
    "            'xgboost__n_estimators': Integer(50, 150)\n",
    "        }\n",
    "    }\n",
    "    pipelines = {\n",
    "        'Linear Regression': Pipeline([\n",
    "            ('linearregression', LinearRegression())\n",
    "        ]),\n",
    "        'Ridge Regression': Pipeline([\n",
    "            ('ridge', Ridge())\n",
    "        ]),\n",
    "        'Lasso Regression': Pipeline([\n",
    "            ('lasso', Lasso())\n",
    "        ]),\n",
    "        'Elastic Net Regression': Pipeline([\n",
    "            ('elasticnet', ElasticNet())\n",
    "        ]),\n",
    "        'LightGBM Regression': Pipeline([\n",
    "            ('lightgbm', LGBMRegressor(\n",
    "                num_leaves=31,\n",
    "                learning_rate=0.01,\n",
    "                n_estimators=100\n",
    "            ))\n",
    "        ]),\n",
    "        'Random Forest Regression': Pipeline([\n",
    "            ('randomforest', RandomForestRegressor())\n",
    "        ]),\n",
    "        'XGBoost Regression': Pipeline([\n",
    "            ('xgboost', xgb.XGBRegressor(use_label_encoder=False, eval_metric='rmse'))\n",
    "        ]),\n",
    "    }\n",
    "\n",
    "    # Remove LightGBM from default_params since we're setting them directly in the pipeline\n",
    "    default_params = {\n",
    "        'Ridge Regression': {\n",
    "            'ridge__alpha': 1.0\n",
    "        },\n",
    "        'Lasso Regression': {\n",
    "            'lasso__alpha': 0.1\n",
    "        },\n",
    "        'Elastic Net Regression': {\n",
    "            'elasticnet__alpha': 0.1,\n",
    "            'elasticnet__l1_ratio': 0.5\n",
    "        },\n",
    "        'Random Forest Regression': {\n",
    "            'randomforest__n_estimators': 100,\n",
    "            'randomforest__max_depth': 10,\n",
    "            'randomforest__min_samples_split': 2\n",
    "        },\n",
    "        'XGBoost Regression': {\n",
    "            'xgboost__learning_rate': 0.01,\n",
    "            'xgboost__max_depth': 3,\n",
    "            'xgboost__n_estimators': 100\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Set default parameters for each pipeline\n",
    "    for name, pipeline in pipelines.items():\n",
    "        if name in default_params:\n",
    "            pipeline.set_params(**default_params[name])\n",
    "\n",
    "    models = {}\n",
    "    for name, pipeline in pipelines.items():\n",
    "        if name in param_spaces:\n",
    "            models[name] = BayesSearchCV(\n",
    "                estimator=pipeline,\n",
    "                search_spaces=param_spaces[name],\n",
    "                cv=3,\n",
    "                scoring='r2',\n",
    "                n_jobs=1,\n",
    "                verbose=1,\n",
    "                n_iter=5,  # Reduced number of iterations\n",
    "                random_state=42\n",
    "            )\n",
    "        else:\n",
    "            models[name] = pipeline\n",
    "\n",
    "    # Training threshold\n",
    "    training_threshold = 7200  # seconds\n",
    "\n",
    "    # List of datasets\n",
    "    datasets = ['dataset1', 'dataset5']\n",
    "\n",
    "    # Results folder\n",
    "    results_dir = \"model_results\"\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "    # Models directory\n",
    "    models_dir = \"models\"\n",
    "    os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "    # Process each dataset\n",
    "    for dataset_name in datasets:\n",
    "        print(f\"\\nProcessing {dataset_name}\")\n",
    "\n",
    "        # Load pre-split data\n",
    "        data_path = f\"/home/dev/project/modelling/preprocessing/results/{dataset_name}\"\n",
    "        try:\n",
    "            train_data = pd.read_csv(os.path.join(data_path, \"train.csv\"))\n",
    "            test_data = pd.read_csv(os.path.join(data_path, \"test.csv\"))\n",
    "            val_data = pd.read_csv(os.path.join(data_path, \"val.csv\"))\n",
    "            print(f\"Successfully loaded pre-split data for {dataset_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading data for {dataset_name}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # drop column 'mssv' if it exists\n",
    "        if 'mssv' in train_data.columns:\n",
    "            train_data = train_data.drop(columns=['mssv'])\n",
    "            test_data = test_data.drop(columns=['mssv'])\n",
    "            val_data = val_data.drop(columns=['mssv'])\n",
    "        \n",
    "        # Define target variable\n",
    "        target_variable = 'diem_hp'\n",
    "        if target_variable not in train_data.columns:\n",
    "            print(f\"Target variable '{target_variable}' not found in training data for dataset '{dataset_name}'. Skipping this dataset.\")\n",
    "            continue\n",
    "\n",
    "        # Separate features and target\n",
    "        X_train = train_data.drop(columns=[target_variable])\n",
    "        y_train = train_data[target_variable]\n",
    "        X_test = test_data.drop(columns=[target_variable])\n",
    "        y_test = test_data[target_variable]\n",
    "        X_val = val_data.drop(columns=[target_variable])\n",
    "        y_val = val_data[target_variable]\n",
    "\n",
    "        # Handle missing values\n",
    "        X_train = X_train.fillna(0)\n",
    "        X_test = X_test.fillna(0)\n",
    "        X_val = X_val.fillna(0)\n",
    "        y_train = y_train.fillna(0)\n",
    "        y_test = y_test.fillna(0)\n",
    "        y_val = y_val.fillna(0)\n",
    "\n",
    "        print(\"All features are now numeric and missing values are handled.\")\n",
    "\n",
    "        # Prepare results storage\n",
    "        results = []\n",
    "\n",
    "        # Train and evaluate each model with default parameters first\n",
    "        for model_name, pipeline in pipelines.items():\n",
    "            if model_name in default_params:\n",
    "                default_model = pipeline.set_params(**default_params[model_name])\n",
    "                print(f\"\\nTraining model: {model_name} (default parameters)\")\n",
    "                train_model(\n",
    "                    model_name=model_name,\n",
    "                    model=default_model,\n",
    "                    X_train=X_train,\n",
    "                    y_train=y_train,\n",
    "                    X_test=X_test,\n",
    "                    y_test=y_test,\n",
    "                    results=results,\n",
    "                    training_threshold=training_threshold,\n",
    "                    dataset_name=dataset_name,\n",
    "                    is_optimized=False,\n",
    "                    X_val=X_val,\n",
    "                    y_val=y_val\n",
    "                )\n",
    "\n",
    "        # Train and evaluate each model with optimization\n",
    "        for model_name, model in models.items():\n",
    "            if model_name in param_spaces:\n",
    "                print(f\"\\nTraining model: {model_name} (with optimization)\")\n",
    "                train_model(\n",
    "                    model_name=model_name,\n",
    "                    model=model,\n",
    "                    X_train=X_train,\n",
    "                    y_train=y_train,\n",
    "                    X_test=X_test,\n",
    "                    y_test=y_test,\n",
    "                    results=results,\n",
    "                    training_threshold=training_threshold,\n",
    "                    dataset_name=dataset_name,\n",
    "                    is_optimized=True,\n",
    "                    X_val=X_val,\n",
    "                    y_val=y_val\n",
    "                )\n",
    "\n",
    "        # Save results to CSV with both default and optimized results\n",
    "        results_df = pd.DataFrame(results)\n",
    "        dataset_results_dir = os.path.join(results_dir, dataset_name)\n",
    "        os.makedirs(dataset_results_dir, exist_ok=True)\n",
    "        results_file = os.path.join(dataset_results_dir, 'model_results_comparison.csv')\n",
    "        results_df.to_csv(results_file, index=False)\n",
    "        print(f\"Results for {dataset_name} saved to {results_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
