{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing dataset1\n",
      "Successfully loaded pre-split data for dataset1\n",
      "All features are now numeric and missing values are handled.\n",
      "\n",
      "Training model: Linear Regression\n",
      "Starting training for Linear Regression...\n",
      "Completed training for Linear Regression in 0.61 seconds.\n",
      "Model Linear Regression trained successfully in 0.61 seconds.\n",
      "MSE: 3.2919359280973275, RMSE: 1.8143692920950045, MAE: 1.2694315353712364, R²: 0.5187536134032071, Adjusted R²: 0.5179535260530577\n",
      "Model Linear Regression saved to models/dataset1/Linear_Regression.pkl\n",
      "\n",
      "Training model: Ridge Regression\n",
      "Starting training for Ridge Regression...\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "Completed training for Ridge Regression in 2.69 seconds.\n",
      "Model Ridge Regression trained successfully in 2.69 seconds.\n",
      "MSE: 3.2918223839399534, RMSE: 1.8143380015697057, MAE: 1.269440415049705, R²: 0.518770212364016, Adjusted R²: 0.5179701526101643\n",
      "Best parameters for Ridge Regression: {'ridge__alpha': 1.0}\n",
      "Model Ridge Regression saved to models/dataset1/Ridge_Regression.pkl\n",
      "\n",
      "Training model: Lasso Regression\n",
      "Starting training for Lasso Regression...\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "Completed training for Lasso Regression in 17.34 seconds.\n",
      "Model Lasso Regression trained successfully in 17.34 seconds.\n",
      "MSE: 3.29479920915117, RMSE: 1.8151581774465744, MAE: 1.273477549062084, R²: 0.5183350318478337, Adjusted R²: 0.5175342485925996\n",
      "Best parameters for Lasso Regression: {'lasso__alpha': 0.01}\n",
      "Model Lasso Regression saved to models/dataset1/Lasso_Regression.pkl\n",
      "\n",
      "Training model: Elastic Net Regression\n",
      "Starting training for Elastic Net Regression...\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Completed training for Elastic Net Regression in 99.72 seconds.\n",
      "Model Elastic Net Regression trained successfully in 99.72 seconds.\n",
      "MSE: 3.2939836814608876, RMSE: 1.8149335198460816, MAE: 1.2733715962589476, R²: 0.5184542534131042, Adjusted R²: 0.5176536683674959\n",
      "Best parameters for Elastic Net Regression: {'elasticnet__alpha': 0.01, 'elasticnet__l1_ratio': 0.9}\n",
      "Model Elastic Net Regression saved to models/dataset1/Elastic_Net_Regression.pkl\n",
      "\n",
      "Training model: LightGBM Regression\n",
      "Starting training for LightGBM Regression...\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 1.588105 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5245\n",
      "[LightGBM] [Info] Number of data points in the train set: 201675, number of used features: 64\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 1.698971 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5253\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 1.668917 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5249\n",
      "[LightGBM] [Info] Start training from score 6.527740\n",
      "[LightGBM] [Info] Number of data points in the train set: 201675, number of used features: 64\n",
      "[LightGBM] [Info] Number of data points in the train set: 201675, number of used features: 64\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 1.764990 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5249\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 1.770853 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5153\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 1.665954 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5231\n",
      "[LightGBM] [Info] Start training from score 6.613190\n",
      "[LightGBM] [Info] Number of data points in the train set: 201675, number of used features: 64\n",
      "[LightGBM] [Info] Start training from score 6.588749\n",
      "[LightGBM] [Info] Number of data points in the train set: 201675, number of used features: 64\n",
      "[LightGBM] [Info] Number of data points in the train set: 201676, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 6.613190\n",
      "[LightGBM] [Info] Start training from score 6.525862\n",
      "[LightGBM] [Info] Start training from score 6.636191\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 1.764887 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5245\n",
      "[LightGBM] [Info] Number of data points in the train set: 201675, number of used features: 64\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 1.784847 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5253\n",
      "[LightGBM] [Info] Number of data points in the train set: 201675, number of used features: 64\n",
      "[LightGBM] [Info] Start training from score 6.527740\n",
      "[LightGBM] [Info] Start training from score 6.588749\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import threading\n",
    "import time\n",
    "import pickle\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "from lightgbm import LGBMRegressor\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# Import BayesSearchCV from scikit-optimize\n",
    "from skopt import BayesSearchCV\n",
    "\n",
    "def train_model(model_name, model, X_train, y_train, X_test, y_test, results, training_threshold, dataset_name):\n",
    "    y_pred = [None]\n",
    "    training_time = [None]\n",
    "    training_completed = [False]\n",
    "\n",
    "    def train():\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            print(f\"Starting training for {model_name}...\")\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred[0] = model.predict(X_test)\n",
    "            training_time[0] = time.time() - start_time\n",
    "            training_completed[0] = True\n",
    "            print(f\"Completed training for {model_name} in {training_time[0]:.2f} seconds.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error training model {model_name}: {e}\")\n",
    "            training_completed[0] = False\n",
    "\n",
    "    thread = threading.Thread(target=train)\n",
    "    thread.start()\n",
    "    thread.join(timeout=training_threshold)\n",
    "\n",
    "    if not training_completed[0]:\n",
    "        print(f\"Model {model_name} exceeded training time ({training_threshold} seconds) or encountered an error.\")\n",
    "        y_pred[0] = np.nan\n",
    "        training_time[0] = np.nan\n",
    "    else:\n",
    "        mse = mean_squared_error(y_test, y_pred[0])\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(y_test, y_pred[0])\n",
    "        r_squared = r2_score(y_test, y_pred[0])\n",
    "\n",
    "        n = len(y_test)\n",
    "        p = X_test.shape[1]\n",
    "        if n > p + 1 and p > 0:\n",
    "            adjusted_r_squared = 1 - (1 - r_squared) * ((n - 1) / (n - p - 1))\n",
    "        else:\n",
    "            adjusted_r_squared = r_squared\n",
    "\n",
    "        print(f\"Model {model_name} trained successfully in {training_time[0]:.2f} seconds.\")\n",
    "        print(f\"MSE: {mse}, RMSE: {rmse}, MAE: {mae}, R²: {r_squared}, Adjusted R²: {adjusted_r_squared}\")\n",
    "\n",
    "        best_params = None\n",
    "        # Check if model is a BayesSearchCV object\n",
    "        if isinstance(model, BayesSearchCV):\n",
    "            best_params = model.best_params_\n",
    "            print(f\"Best parameters for {model_name}: {best_params}\")\n",
    "\n",
    "        models_dir = os.path.join(\"models\", dataset_name)\n",
    "        os.makedirs(models_dir, exist_ok=True)\n",
    "        model_filename = f\"{model_name.replace(' ', '_')}.pkl\"\n",
    "        model_filepath = os.path.join(models_dir, model_filename)\n",
    "        with open(model_filepath, 'wb') as f:\n",
    "            pickle.dump(model, f)\n",
    "        print(f\"Model {model_name} saved to {model_filepath}\")\n",
    "\n",
    "        result = {\n",
    "            'Model': model_name,\n",
    "            'Dataset': dataset_name,\n",
    "            'Training Time (s)': training_time[0],\n",
    "            'MSE': mse,\n",
    "            'RMSE': rmse,\n",
    "            'MAE': mae,\n",
    "            'R2 Score': r_squared,\n",
    "            'Adjusted R2 Score': adjusted_r_squared\n",
    "        }\n",
    "        if best_params:\n",
    "            result['Best Params'] = str(best_params)\n",
    "        results.append(result)\n",
    "\n",
    "def main():\n",
    "    # Define parameter search spaces for each model (same as param_grids, just used as search_spaces now)\n",
    "    param_spaces = {\n",
    "        'Ridge Regression': {\n",
    "            'ridge__alpha': [0.1, 1.0, 10.0, 100.0]\n",
    "        },\n",
    "        'Lasso Regression': {\n",
    "            'lasso__alpha': [0.01, 0.1, 1.0, 10.0]\n",
    "        },\n",
    "        'Elastic Net Regression': {\n",
    "            'elasticnet__alpha': [0.01, 0.1, 1.0, 10.0],\n",
    "            'elasticnet__l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "        },\n",
    "        'LightGBM Regression': {\n",
    "            'lightgbm__num_leaves': [31, 50, 70],\n",
    "            'lightgbm__learning_rate': [0.01, 0.05, 0.1],\n",
    "            'lightgbm__n_estimators': [100, 200, 500]\n",
    "        },\n",
    "        'Random Forest Regression': {\n",
    "            'randomforest__n_estimators': [100, 200, 500],\n",
    "            'randomforest__max_depth': [10, 20, None],\n",
    "            'randomforest__min_samples_split': [2, 5, 10]\n",
    "        },\n",
    "        'Support Vector Regression': {\n",
    "            'svr__C': [0.1, 1, 10],\n",
    "            'svr__epsilon': [0.1, 0.2, 0.5],\n",
    "            'svr__kernel': ['linear', 'rbf']\n",
    "        },\n",
    "        'XGBoost Regression': {\n",
    "            'xgboost__learning_rate': [0.01, 0.05, 0.1],\n",
    "            'xgboost__max_depth': [3, 5, 7],\n",
    "            'xgboost__n_estimators': [100, 200, 500]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Define pipelines for each model\n",
    "    pipelines = {\n",
    "        'Linear Regression': Pipeline([\n",
    "            ('linearregression', LinearRegression())\n",
    "        ]),\n",
    "        'Ridge Regression': Pipeline([\n",
    "            ('ridge', Ridge())\n",
    "        ]),\n",
    "        'Lasso Regression': Pipeline([\n",
    "            ('lasso', Lasso())\n",
    "        ]),\n",
    "        'Elastic Net Regression': Pipeline([\n",
    "            ('elasticnet', ElasticNet())\n",
    "        ]),\n",
    "        'LightGBM Regression': Pipeline([\n",
    "            ('lightgbm', LGBMRegressor())\n",
    "        ]),\n",
    "        'Random Forest Regression': Pipeline([\n",
    "            ('randomforest', RandomForestRegressor())\n",
    "        ]),\n",
    "        'XGBoost Regression': Pipeline([\n",
    "            ('xgboost', xgb.XGBRegressor(use_label_encoder=False, eval_metric='rmse'))\n",
    "        ]),\n",
    "    }\n",
    "\n",
    "    # Create BayesSearchCV objects\n",
    "    models = {}\n",
    "    for name, pipeline in pipelines.items():\n",
    "        if name in param_spaces:\n",
    "            models[name] = BayesSearchCV(\n",
    "                estimator=pipeline,\n",
    "                search_spaces=param_spaces[name],\n",
    "                cv=5,\n",
    "                scoring='r2',\n",
    "                n_jobs=-1,\n",
    "                verbose=1,\n",
    "                n_iter=30, \n",
    "                random_state=42\n",
    "            )\n",
    "        else:\n",
    "            # No hyperparameter tuning for Linear Regression\n",
    "            models[name] = pipeline\n",
    "\n",
    "    # Training threshold\n",
    "    training_threshold = 7200  # seconds\n",
    "\n",
    "    # List of datasets\n",
    "    datasets = ['dataset1', 'dataset5']\n",
    "\n",
    "    # Results folder\n",
    "    results_dir = \"model_results\"\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "    # Models directory\n",
    "    models_dir = \"models\"\n",
    "    os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "    # Process each dataset\n",
    "    for dataset_name in datasets:\n",
    "        print(f\"\\nProcessing {dataset_name}\")\n",
    "\n",
    "        # Load pre-split data\n",
    "        data_path = f\"/home/dev/project/modelling/preprocessing/results/{dataset_name}\"\n",
    "        try:\n",
    "            train_data = pd.read_csv(os.path.join(data_path, \"train.csv\"))\n",
    "            test_data = pd.read_csv(os.path.join(data_path, \"test.csv\"))\n",
    "            val_data = pd.read_csv(os.path.join(data_path, \"val.csv\"))\n",
    "            print(f\"Successfully loaded pre-split data for {dataset_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading data for {dataset_name}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # drop column 'mssv' if it exists\n",
    "        if 'mssv' in train_data.columns:\n",
    "            train_data = train_data.drop(columns=['mssv'])\n",
    "            test_data = test_data.drop(columns=['mssv'])\n",
    "            val_data = val_data.drop(columns=['mssv'])\n",
    "        \n",
    "        # Define target variable\n",
    "        target_variable = 'diem_hp'\n",
    "        if target_variable not in train_data.columns:\n",
    "            print(f\"Target variable '{target_variable}' not found in training data for dataset '{dataset_name}'. Skipping this dataset.\")\n",
    "            continue\n",
    "\n",
    "        # Separate features and target\n",
    "        X_train = train_data.drop(columns=[target_variable])\n",
    "        y_train = train_data[target_variable]\n",
    "        X_test = test_data.drop(columns=[target_variable])\n",
    "        y_test = test_data[target_variable]\n",
    "        X_val = val_data.drop(columns=[target_variable])\n",
    "        y_val = val_data[target_variable]\n",
    "\n",
    "        # Handle missing values\n",
    "        X_train = X_train.fillna(0)\n",
    "        X_test = X_test.fillna(0)\n",
    "        X_val = X_val.fillna(0)\n",
    "        y_train = y_train.fillna(0)\n",
    "        y_test = y_test.fillna(0)\n",
    "        y_val = y_val.fillna(0)\n",
    "\n",
    "        print(\"All features are now numeric and missing values are handled.\")\n",
    "\n",
    "        # Prepare results storage\n",
    "        results = []\n",
    "\n",
    "        # Train and evaluate each model\n",
    "        for model_name, model in models.items():\n",
    "            print(f\"\\nTraining model: {model_name}\")\n",
    "            train_model(\n",
    "                model_name=model_name,\n",
    "                model=model,\n",
    "                X_train=X_train,\n",
    "                y_train=y_train,\n",
    "                X_test=X_test,\n",
    "                y_test=y_test,\n",
    "                results=results,\n",
    "                training_threshold=training_threshold,\n",
    "                dataset_name=dataset_name\n",
    "            )\n",
    "\n",
    "        # Save results to CSV\n",
    "        results_df = pd.DataFrame(results)\n",
    "        dataset_results_dir = os.path.join(results_dir, dataset_name)\n",
    "        os.makedirs(dataset_results_dir, exist_ok=True)\n",
    "        results_file = os.path.join(dataset_results_dir, 'model_results.csv')\n",
    "        results_df.to_csv(results_file, index=False)\n",
    "        print(f\"Results for {dataset_name} saved to {results_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
