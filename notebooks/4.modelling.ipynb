{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import threading\n",
    "import time\n",
    "import pickle  # Add this import\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "import xgboost as xgb\n",
    "import sys\n",
    "sys.stdout.reconfigure(line_buffering=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Define the column cleaning functions\n",
    "def clean_column_names(df):\n",
    "    \"\"\"\n",
    "    Clean column names by:\n",
    "    - Replacing non-alphanumeric characters with underscores.\n",
    "    - Ensuring column names start with a letter.\n",
    "    - Making column names unique.\n",
    "    \"\"\"\n",
    "    # Replace any sequence of non-word characters with a single underscore\n",
    "    df.columns = [\n",
    "        re.sub(r'\\W+', '_', col).strip('_') for col in df.columns\n",
    "    ]\n",
    "\n",
    "    # Ensure column names start with a letter by prefixing with 'f_' if necessary\n",
    "    df.columns = [\n",
    "        col if re.match(r'^[A-Za-z]', col) else f'f_{col}' for col in df.columns\n",
    "    ]\n",
    "\n",
    "    # Ensure uniqueness by appending suffixes to duplicate names\n",
    "    seen = {}\n",
    "    new_columns = []\n",
    "    for col in df.columns:\n",
    "        if col in seen:\n",
    "            seen[col] += 1\n",
    "            new_columns.append(f\"{col}_{seen[col]}\")\n",
    "        else:\n",
    "            seen[col] = 0\n",
    "            new_columns.append(col)\n",
    "    df.columns = new_columns\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Define the model training function\n",
    "def train_model(model_name, model, X_train, y_train, X_test, y_test, results, training_threshold, dataset_name):\n",
    "    y_pred = [None]\n",
    "    training_time = [None]\n",
    "    training_completed = [False]\n",
    "\n",
    "    def train():\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            print(f\"Starting training for {model_name}...\")\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred[0] = model.predict(X_test)\n",
    "            training_time[0] = time.time() - start_time\n",
    "            training_completed[0] = True\n",
    "            print(f\"Completed training for {model_name} in {training_time[0]:.2f} seconds.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error training model {model_name}: {e}\")\n",
    "            training_completed[0] = False\n",
    "\n",
    "    thread = threading.Thread(target=train)\n",
    "    thread.start()\n",
    "    thread.join(timeout=training_threshold)\n",
    "\n",
    "    if not training_completed[0]:\n",
    "        print(f\"Model {model_name} exceeded training time ({training_threshold} seconds) or encountered an error.\")\n",
    "        y_pred[0] = np.nan\n",
    "        training_time[0] = np.nan\n",
    "    else:\n",
    "        mse = mean_squared_error(y_test, y_pred[0])\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(y_test, y_pred[0])\n",
    "        r_squared = r2_score(y_test, y_pred[0])\n",
    "\n",
    "        n = len(y_test)\n",
    "        p = X_test.shape[1]\n",
    "        if n > p + 1 and p > 0:\n",
    "            adjusted_r_squared = 1 - (1 - r_squared) * ((n - 1) / (n - p - 1))\n",
    "        else:\n",
    "            adjusted_r_squared = r_squared\n",
    "\n",
    "        print(f\"Model {model_name} trained successfully in {training_time[0]:.2f} seconds.\")\n",
    "        print(f\"MSE: {mse}, RMSE: {rmse}, MAE: {mae}, R²: {r_squared}, Adjusted R²: {adjusted_r_squared}\")\n",
    "\n",
    "        best_params = None\n",
    "        if isinstance(model, GridSearchCV):\n",
    "            best_params = model.best_params_\n",
    "            print(f\"Best parameters for {model_name}: {best_params}\")\n",
    "\n",
    "        models_dir = os.path.join(\"models\", dataset_name)\n",
    "        os.makedirs(models_dir, exist_ok=True)\n",
    "        model_filename = f\"{model_name.replace(' ', '_')}.pkl\"\n",
    "        model_filepath = os.path.join(models_dir, model_filename)\n",
    "        with open(model_filepath, 'wb') as f:\n",
    "            pickle.dump(model, f)\n",
    "        print(f\"Model {model_name} saved to {model_filepath}\")\n",
    "\n",
    "        result = {\n",
    "            'Model': model_name,\n",
    "            'Dataset': dataset_name,\n",
    "            'Training Time (s)': training_time[0],\n",
    "            'MSE': mse,\n",
    "            'RMSE': rmse,\n",
    "            'MAE': mae,\n",
    "            'R2 Score': r_squared,\n",
    "            'Adjusted R2 Score': adjusted_r_squared\n",
    "        }\n",
    "        if best_params:\n",
    "            result['Best Params'] = str(best_params)\n",
    "        results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Main Script\n",
    "def main():\n",
    "    # Define parameter grids for each model\n",
    "    param_grids = {\n",
    "        'Ridge Regression': {\n",
    "            'ridge__alpha': [0.1, 1.0, 10.0, 100.0]\n",
    "        },\n",
    "        'Lasso Regression': {\n",
    "            'lasso__alpha': [0.01, 0.1, 1.0, 10.0]\n",
    "        },\n",
    "        'Elastic Net Regression': {\n",
    "            'elasticnet__alpha': [0.01, 0.1, 1.0, 10.0],\n",
    "            'elasticnet__l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "        },\n",
    "        'LightGBM Regression': {\n",
    "            'lightgbm__num_leaves': [31, 50, 70],\n",
    "            'lightgbm__learning_rate': [0.01, 0.05, 0.1],\n",
    "            'lightgbm__n_estimators': [100, 200, 500]\n",
    "        },\n",
    "        'Random Forest Regression': {\n",
    "            'randomforest__n_estimators': [100, 200, 500],\n",
    "            'randomforest__max_depth': [10, 20, None],\n",
    "            'randomforest__min_samples_split': [2, 5, 10]\n",
    "        },\n",
    "        'Support Vector Regression': {\n",
    "            'svr__C': [0.1, 1, 10],\n",
    "            'svr__epsilon': [0.1, 0.2, 0.5],\n",
    "            'svr__kernel': ['linear', 'rbf']\n",
    "        },\n",
    "        'XGBoost Regression': {\n",
    "            'xgboost__learning_rate': [0.01, 0.05, 0.1],\n",
    "            'xgboost__max_depth': [3, 5, 7],\n",
    "            'xgboost__n_estimators': [100, 200, 500]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Define pipelines for each model\n",
    "    pipelines = {\n",
    "        'Linear Regression': Pipeline([\n",
    "            ('linearregression', LinearRegression())\n",
    "        ]),\n",
    "        'Ridge Regression': Pipeline([\n",
    "            ('ridge', Ridge())\n",
    "        ]),\n",
    "        'Lasso Regression': Pipeline([\n",
    "            ('lasso', Lasso())\n",
    "        ]),\n",
    "        'Elastic Net Regression': Pipeline([\n",
    "            ('elasticnet', ElasticNet())\n",
    "        ]),\n",
    "        'LightGBM Regression': Pipeline([\n",
    "            ('lightgbm', LGBMRegressor())\n",
    "        ]),\n",
    "        'Random Forest Regression': Pipeline([\n",
    "            ('randomforest', RandomForestRegressor())\n",
    "        ]),\n",
    "        # 'Support Vector Regression': Pipeline([\n",
    "        #     ('svr', SVR())\n",
    "        # ]),\n",
    "        'XGBoost Regression': Pipeline([\n",
    "            ('xgboost', xgb.XGBRegressor(use_label_encoder=False, eval_metric='rmse'))\n",
    "        ]),\n",
    "    }\n",
    "\n",
    "    # Create GridSearchCV objects\n",
    "    models = {}\n",
    "    for name, pipeline in pipelines.items():\n",
    "        if name in param_grids:\n",
    "            models[name] = GridSearchCV(\n",
    "                estimator=pipeline,\n",
    "                param_grid=param_grids[name],\n",
    "                cv=5,\n",
    "                scoring='r2',\n",
    "                n_jobs=-1,\n",
    "                verbose=1\n",
    "            )\n",
    "        else:\n",
    "            models[name] = pipeline  # Linear Regression has no hyperparameters\n",
    "\n",
    "    # Training threshold\n",
    "    training_threshold = 7200  # seconds\n",
    "\n",
    "    # List of preprocessed datasets\n",
    "    datasets = ['dataset5.csv', 'dataset4.csv', 'dataset3.csv', 'dataset2.csv', 'dataset1.csv']\n",
    "\n",
    "    # Results folder\n",
    "    results_dir = \"model_results\"\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "    # Models directory\n",
    "    models_dir = \"models\"\n",
    "    os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "    # Process each dataset\n",
    "    for dataset_name in datasets:\n",
    "        print(f\"\\nProcessing {dataset_name}\")\n",
    "\n",
    "        # Read dataset\n",
    "        try:\n",
    "            data = pd.read_csv(dataset_name)\n",
    "            print(f\"Successfully read {dataset_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {dataset_name}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Clean column names\n",
    "        data = clean_column_names(data)\n",
    "        print(\"Column names after cleaning:\", data.columns.tolist())\n",
    "\n",
    "        # Verify column names\n",
    "        problematic_columns = verify_column_names(data)\n",
    "        if problematic_columns:\n",
    "            print(f\"Problematic columns after cleaning: {problematic_columns}\")\n",
    "            print(\"Further cleaning or renaming may be required.\")\n",
    "        else:\n",
    "            print(\"All column names are clean and compatible with models.\")\n",
    "\n",
    "        # Identify numerical and categorical columns\n",
    "        numerical = data.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "        categorical = data.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "        print(f\"Numerical columns: {numerical}\")\n",
    "        print(f\"Categorical columns: {categorical}\")\n",
    "\n",
    "        # Define target variable\n",
    "        target_variable = 'diem_hp'\n",
    "        if target_variable not in data.columns:\n",
    "            print(f\"Target variable '{target_variable}' not found in dataset '{dataset_name}'. Skipping this dataset.\")\n",
    "            continue\n",
    "        if target_variable in numerical:\n",
    "            numerical.remove(target_variable)\n",
    "        if target_variable in categorical:\n",
    "            categorical.remove(target_variable)\n",
    "\n",
    "        # Define features and target\n",
    "        X = data.drop(columns=[target_variable])\n",
    "        y = data[target_variable]\n",
    "\n",
    "        # Specify the group column\n",
    "        group_column = 'hocky_monhoc_count'\n",
    "\n",
    "        if group_column not in data.columns:\n",
    "            print(f\"Group column '{group_column}' not found in dataset '{dataset_name}'.\")\n",
    "            print(\"Proceeding with a standard train-test split without grouping.\")\n",
    "            try:\n",
    "                X_train, X_test, y_train, y_test = train_test_split(\n",
    "                    X, y, test_size=0.2, random_state=42, shuffle=True\n",
    "                )\n",
    "                print(f\"Data split into Training ({len(X_train)} samples) and Testing ({len(X_test)} samples) sets.\")\n",
    "            except ValueError as e:\n",
    "                print(f\"Error splitting data for dataset '{dataset_name}': {e}\")\n",
    "                continue\n",
    "        else:\n",
    "            print(f\"\\nGroup column '{group_column}' found. Proceeding with grouped split.\")\n",
    "            print(f\"Initial data shape: {data.shape}\")\n",
    "            \n",
    "            # Split data using split_data_by_group\n",
    "            train_set, val_set, test_set = split_data_by_group(\n",
    "                data, \n",
    "                group_col=group_column,\n",
    "                train_ratio=0.8,\n",
    "                val_ratio=0.2,\n",
    "                test_ratio=None\n",
    "            )\n",
    "            \n",
    "            if len(train_set) == 0 or len(test_set) == 0:\n",
    "                print(\"\\nFallback to standard train-test split due to empty sets...\")\n",
    "                X_train, X_test, y_train, y_test = train_test_split(\n",
    "                    X, y, test_size=0.2, random_state=42, shuffle=True\n",
    "                )\n",
    "            else:\n",
    "                # Define features and targets\n",
    "                X_train = train_set.drop(columns=[target_variable])\n",
    "                y_train = train_set[target_variable]\n",
    "                X_val = val_set.drop(columns=[target_variable])\n",
    "                y_val = val_set[target_variable]\n",
    "                X_test = test_set.drop(columns=[target_variable])\n",
    "                y_test = test_set[target_variable]\n",
    "\n",
    "                print(f\"\\nFinal shapes:\")\n",
    "                print(f\"Training set: {X_train.shape}\")\n",
    "                print(f\"Validation set: {X_val.shape}\")\n",
    "                print(f\"Testing set: {X_test.shape}\")\n",
    "\n",
    "        # Encode categorical variables\n",
    "        if group_column in categorical:\n",
    "            categorical.remove(group_column)\n",
    "\n",
    "        use_validation = 'X_val' in locals()\n",
    "\n",
    "        if use_validation:\n",
    "            X_to_encode = [X_train, X_val, X_test]\n",
    "            y_to_encode = [y_train, y_val, y_test]\n",
    "        else:\n",
    "            X_to_encode = [X_train, X_test]\n",
    "            y_to_encode = [y_train, y_test]\n",
    "\n",
    "        for col in categorical:\n",
    "            for df in X_to_encode:\n",
    "                if col in df.columns:\n",
    "                    df[col] = df[col].astype('category').cat.codes\n",
    "            print(f\"Label encoded categorical column: {col}\")\n",
    "\n",
    "        if use_validation:\n",
    "            X_train, X_val, X_test = X_to_encode\n",
    "            y_train, y_val, y_test = y_to_encode\n",
    "        else:\n",
    "            X_train, X_test = X_to_encode\n",
    "            y_train, y_test = y_to_encode\n",
    "\n",
    "        print(\"All categorical features are now label encoded.\")\n",
    "\n",
    "        # Handle missing values\n",
    "        if use_validation:\n",
    "            X_train = X_train.fillna(0)\n",
    "            X_val = X_val.fillna(0)\n",
    "            X_test = X_test.fillna(0)\n",
    "            y_train = y_train.fillna(0)\n",
    "            y_val = y_val.fillna(0)\n",
    "            y_test = y_test.fillna(0)\n",
    "        else:\n",
    "            X_train = X_train.fillna(0)\n",
    "            X_test = X_test.fillna(0)\n",
    "            y_train = y_train.fillna(0)\n",
    "            y_test = y_test.fillna(0)\n",
    "\n",
    "        print(\"All features are now numeric and missing values are handled.\")\n",
    "\n",
    "        # Prepare results storage\n",
    "        results = []\n",
    "\n",
    "        # Train and evaluate each model\n",
    "        for model_name, model in models.items():\n",
    "            print(f\"\\nTraining model: {model_name}\")\n",
    "\n",
    "            if use_validation and model_name in ['LightGBM Regression', 'Random Forest Regression', 'XGBoost Regression']:\n",
    "                # If using validation, you might want to include it in GridSearchCV\n",
    "                train_model(\n",
    "                    model_name=model_name,\n",
    "                    model=model,\n",
    "                    X_train=X_train,\n",
    "                    y_train=y_train,\n",
    "                    X_test=X_test,\n",
    "                    y_test=y_test,\n",
    "                    results=results,\n",
    "                    training_threshold=training_threshold,\n",
    "                    dataset_name=dataset_name\n",
    "                )\n",
    "            else:\n",
    "                train_model(\n",
    "                    model_name=model_name,\n",
    "                    model=model,\n",
    "                    X_train=X_train,\n",
    "                    y_train=y_train,\n",
    "                    X_test=X_test,\n",
    "                    y_test=y_test,\n",
    "                    results=results,\n",
    "                    training_threshold=training_threshold,\n",
    "                    dataset_name=dataset_name\n",
    "                )\n",
    "\n",
    "        # Save results to CSV\n",
    "        results_df = pd.DataFrame(results)\n",
    "        dataset_results_dir = os.path.join(results_dir, os.path.splitext(dataset_name)[0])\n",
    "        os.makedirs(dataset_results_dir, exist_ok=True)\n",
    "        results_file = os.path.join(dataset_results_dir, 'model_results.csv')\n",
    "        results_df.to_csv(results_file, index=False)\n",
    "        print(f\"Results for {dataset_name} saved to {results_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Main Script\n",
    "def main():\n",
    "    # Define parameter grids for each model\n",
    "    param_grids = {\n",
    "        'Ridge Regression': {\n",
    "            'ridge__alpha': [0.1, 1.0, 10.0, 100.0]\n",
    "        },\n",
    "        'Lasso Regression': {\n",
    "            'lasso__alpha': [0.01, 0.1, 1.0, 10.0]\n",
    "        },\n",
    "        'Elastic Net Regression': {\n",
    "            'elasticnet__alpha': [0.01, 0.1, 1.0, 10.0],\n",
    "            'elasticnet__l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "        },\n",
    "        'LightGBM Regression': {\n",
    "            'lightgbm__num_leaves': [31, 50, 70],\n",
    "            'lightgbm__learning_rate': [0.01, 0.05, 0.1],\n",
    "            'lightgbm__n_estimators': [100, 200, 500]\n",
    "        },\n",
    "        'Random Forest Regression': {\n",
    "            'randomforest__n_estimators': [100, 200, 500],\n",
    "            'randomforest__max_depth': [10, 20, None],\n",
    "            'randomforest__min_samples_split': [2, 5, 10]\n",
    "        },\n",
    "        'Support Vector Regression': {\n",
    "            'svr__C': [0.1, 1, 10],\n",
    "            'svr__epsilon': [0.1, 0.2, 0.5],\n",
    "            'svr__kernel': ['linear', 'rbf']\n",
    "        },\n",
    "        'XGBoost Regression': {\n",
    "            'xgboost__learning_rate': [0.01, 0.05, 0.1],\n",
    "            'xgboost__max_depth': [3, 5, 7],\n",
    "            'xgboost__n_estimators': [100, 200, 500]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Define pipelines for each model\n",
    "    pipelines = {\n",
    "        'Linear Regression': Pipeline([\n",
    "            ('linearregression', LinearRegression())\n",
    "        ]),\n",
    "        'Ridge Regression': Pipeline([\n",
    "            ('ridge', Ridge())\n",
    "        ]),\n",
    "        'Lasso Regression': Pipeline([\n",
    "            ('lasso', Lasso())\n",
    "        ]),\n",
    "        'Elastic Net Regression': Pipeline([\n",
    "            ('elasticnet', ElasticNet())\n",
    "        ]),\n",
    "        'LightGBM Regression': Pipeline([\n",
    "            ('lightgbm', LGBMRegressor())\n",
    "        ]),\n",
    "        'Random Forest Regression': Pipeline([\n",
    "            ('randomforest', RandomForestRegressor())\n",
    "        ]),\n",
    "        # 'Support Vector Regression': Pipeline([\n",
    "        #     ('svr', SVR())\n",
    "        # ]),\n",
    "        'XGBoost Regression': Pipeline([\n",
    "            ('xgboost', xgb.XGBRegressor(use_label_encoder=False, eval_metric='rmse'))\n",
    "        ]),\n",
    "    }\n",
    "    \n",
    "    # Create GridSearchCV objects\n",
    "    models = {}\n",
    "    for name, pipeline in pipelines.items():\n",
    "        if name in param_grids:\n",
    "            models[name] = GridSearchCV(\n",
    "                estimator=pipeline,\n",
    "                param_grid=param_grids[name],\n",
    "                cv=5,\n",
    "                scoring='r2',\n",
    "                n_jobs=-1,\n",
    "                verbose=1\n",
    "            )\n",
    "        else:\n",
    "            models[name] = pipeline  # Linear Regression has no hyperparameters\n",
    "\n",
    "    # Training threshold\n",
    "    training_threshold = 7200 \n",
    "    \n",
    "    # Import train, test, val from folder results\n",
    "    train1 = pd.read_csv('/results/dataset1/train.csv')\n",
    "    test1 = pd.read_csv('/results/dataset1/test.csv')\n",
    "    val1 = pd.read_csv('/results/dataset1/val.csv')\n",
    "    \n",
    "    train5 = pd.read_csv('/results/dataset5/train.csv')\n",
    "    test5 = pd.read_csv('/results/dataset5/test.csv')\n",
    "    val5 = pd.read_csv('/results/dataset5/val.csv')\n",
    "    \n",
    "    # Results folder\n",
    "    results_dir = \"model_results\"\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "    # Models directory\n",
    "    models_dir = \"models\"\n",
    "    os.makedirs(models_dir, exist_ok=True)\n",
    "    \n",
    "    # Process each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing dataset1\n",
      "Successfully loaded pre-split data for dataset1\n",
      "All features are now numeric and missing values are handled.\n",
      "\n",
      "Training model: Linear Regression\n",
      "Starting training for Linear Regression...\n",
      "Completed training for Linear Regression in 0.61 seconds.\n",
      "Model Linear Regression trained successfully in 0.61 seconds.\n",
      "MSE: 3.2919359280973275, RMSE: 1.8143692920950045, MAE: 1.2694315353712364, R²: 0.5187536134032071, Adjusted R²: 0.5179535260530577\n",
      "Model Linear Regression saved to models/dataset1/Linear_Regression.pkl\n",
      "\n",
      "Training model: Ridge Regression\n",
      "Starting training for Ridge Regression...\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "Completed training for Ridge Regression in 2.69 seconds.\n",
      "Model Ridge Regression trained successfully in 2.69 seconds.\n",
      "MSE: 3.2918223839399534, RMSE: 1.8143380015697057, MAE: 1.269440415049705, R²: 0.518770212364016, Adjusted R²: 0.5179701526101643\n",
      "Best parameters for Ridge Regression: {'ridge__alpha': 1.0}\n",
      "Model Ridge Regression saved to models/dataset1/Ridge_Regression.pkl\n",
      "\n",
      "Training model: Lasso Regression\n",
      "Starting training for Lasso Regression...\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "Completed training for Lasso Regression in 17.34 seconds.\n",
      "Model Lasso Regression trained successfully in 17.34 seconds.\n",
      "MSE: 3.29479920915117, RMSE: 1.8151581774465744, MAE: 1.273477549062084, R²: 0.5183350318478337, Adjusted R²: 0.5175342485925996\n",
      "Best parameters for Lasso Regression: {'lasso__alpha': 0.01}\n",
      "Model Lasso Regression saved to models/dataset1/Lasso_Regression.pkl\n",
      "\n",
      "Training model: Elastic Net Regression\n",
      "Starting training for Elastic Net Regression...\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Completed training for Elastic Net Regression in 99.72 seconds.\n",
      "Model Elastic Net Regression trained successfully in 99.72 seconds.\n",
      "MSE: 3.2939836814608876, RMSE: 1.8149335198460816, MAE: 1.2733715962589476, R²: 0.5184542534131042, Adjusted R²: 0.5176536683674959\n",
      "Best parameters for Elastic Net Regression: {'elasticnet__alpha': 0.01, 'elasticnet__l1_ratio': 0.9}\n",
      "Model Elastic Net Regression saved to models/dataset1/Elastic_Net_Regression.pkl\n",
      "\n",
      "Training model: LightGBM Regression\n",
      "Starting training for LightGBM Regression...\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 1.588105 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5245\n",
      "[LightGBM] [Info] Number of data points in the train set: 201675, number of used features: 64\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 1.698971 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5253\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 1.668917 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5249\n",
      "[LightGBM] [Info] Start training from score 6.527740\n",
      "[LightGBM] [Info] Number of data points in the train set: 201675, number of used features: 64\n",
      "[LightGBM] [Info] Number of data points in the train set: 201675, number of used features: 64\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 1.764990 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5249\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 1.770853 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5153\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 1.665954 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5231\n",
      "[LightGBM] [Info] Start training from score 6.613190\n",
      "[LightGBM] [Info] Number of data points in the train set: 201675, number of used features: 64\n",
      "[LightGBM] [Info] Start training from score 6.588749\n",
      "[LightGBM] [Info] Number of data points in the train set: 201675, number of used features: 64\n",
      "[LightGBM] [Info] Number of data points in the train set: 201676, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 6.613190\n",
      "[LightGBM] [Info] Start training from score 6.525862\n",
      "[LightGBM] [Info] Start training from score 6.636191\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 1.764887 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5245\n",
      "[LightGBM] [Info] Number of data points in the train set: 201675, number of used features: 64\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 1.784847 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5253\n",
      "[LightGBM] [Info] Number of data points in the train set: 201675, number of used features: 64\n",
      "[LightGBM] [Info] Start training from score 6.527740\n",
      "[LightGBM] [Info] Start training from score 6.588749\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import threading\n",
    "import time\n",
    "import pickle  # Add this import\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# 1. Define the model training function\n",
    "def train_model(model_name, model, X_train, y_train, X_test, y_test, results, training_threshold, dataset_name):\n",
    "    y_pred = [None]\n",
    "    training_time = [None]\n",
    "    training_completed = [False]\n",
    "\n",
    "    def train():\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            print(f\"Starting training for {model_name}...\")\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred[0] = model.predict(X_test)\n",
    "            training_time[0] = time.time() - start_time\n",
    "            training_completed[0] = True\n",
    "            print(f\"Completed training for {model_name} in {training_time[0]:.2f} seconds.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error training model {model_name}: {e}\")\n",
    "            training_completed[0] = False\n",
    "\n",
    "    thread = threading.Thread(target=train)\n",
    "    thread.start()\n",
    "    thread.join(timeout=training_threshold)\n",
    "\n",
    "    if not training_completed[0]:\n",
    "        print(f\"Model {model_name} exceeded training time ({training_threshold} seconds) or encountered an error.\")\n",
    "        y_pred[0] = np.nan\n",
    "        training_time[0] = np.nan\n",
    "    else:\n",
    "        mse = mean_squared_error(y_test, y_pred[0])\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(y_test, y_pred[0])\n",
    "        r_squared = r2_score(y_test, y_pred[0])\n",
    "\n",
    "        n = len(y_test)\n",
    "        p = X_test.shape[1]\n",
    "        if n > p + 1 and p > 0:\n",
    "            adjusted_r_squared = 1 - (1 - r_squared) * ((n - 1) / (n - p - 1))\n",
    "        else:\n",
    "            adjusted_r_squared = r_squared\n",
    "\n",
    "        print(f\"Model {model_name} trained successfully in {training_time[0]:.2f} seconds.\")\n",
    "        print(f\"MSE: {mse}, RMSE: {rmse}, MAE: {mae}, R²: {r_squared}, Adjusted R²: {adjusted_r_squared}\")\n",
    "\n",
    "        best_params = None\n",
    "        if isinstance(model, GridSearchCV):\n",
    "            best_params = model.best_params_\n",
    "            print(f\"Best parameters for {model_name}: {best_params}\")\n",
    "\n",
    "        models_dir = os.path.join(\"models\", dataset_name)\n",
    "        os.makedirs(models_dir, exist_ok=True)\n",
    "        model_filename = f\"{model_name.replace(' ', '_')}.pkl\"\n",
    "        model_filepath = os.path.join(models_dir, model_filename)\n",
    "        with open(model_filepath, 'wb') as f:\n",
    "            pickle.dump(model, f)\n",
    "        print(f\"Model {model_name} saved to {model_filepath}\")\n",
    "\n",
    "        result = {\n",
    "            'Model': model_name,\n",
    "            'Dataset': dataset_name,\n",
    "            'Training Time (s)': training_time[0],\n",
    "            'MSE': mse,\n",
    "            'RMSE': rmse,\n",
    "            'MAE': mae,\n",
    "            'R2 Score': r_squared,\n",
    "            'Adjusted R2 Score': adjusted_r_squared\n",
    "        }\n",
    "        if best_params:\n",
    "            result['Best Params'] = str(best_params)\n",
    "        results.append(result)\n",
    "\n",
    "# 2. Main Script\n",
    "def main():\n",
    "    # Define parameter grids for each model\n",
    "    param_grids = {\n",
    "        'Ridge Regression': {\n",
    "            'ridge__alpha': [0.1, 1.0, 10.0, 100.0]\n",
    "        },\n",
    "        'Lasso Regression': {\n",
    "            'lasso__alpha': [0.01, 0.1, 1.0, 10.0]\n",
    "        },\n",
    "        'Elastic Net Regression': {\n",
    "            'elasticnet__alpha': [0.01, 0.1, 1.0, 10.0],\n",
    "            'elasticnet__l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "        },\n",
    "        'LightGBM Regression': {\n",
    "            'lightgbm__num_leaves': [31, 50, 70],\n",
    "            'lightgbm__learning_rate': [0.01, 0.05, 0.1],\n",
    "            'lightgbm__n_estimators': [100, 200, 500]\n",
    "        },\n",
    "        'Random Forest Regression': {\n",
    "            'randomforest__n_estimators': [100, 200, 500],\n",
    "            'randomforest__max_depth': [10, 20, None],\n",
    "            'randomforest__min_samples_split': [2, 5, 10]\n",
    "        },\n",
    "        'Support Vector Regression': {\n",
    "            'svr__C': [0.1, 1, 10],\n",
    "            'svr__epsilon': [0.1, 0.2, 0.5],\n",
    "            'svr__kernel': ['linear', 'rbf']\n",
    "        },\n",
    "        'XGBoost Regression': {\n",
    "            'xgboost__learning_rate': [0.01, 0.05, 0.1],\n",
    "            'xgboost__max_depth': [3, 5, 7],\n",
    "            'xgboost__n_estimators': [100, 200, 500]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Define pipelines for each model\n",
    "    pipelines = {\n",
    "        'Linear Regression': Pipeline([\n",
    "            ('linearregression', LinearRegression())\n",
    "        ]),\n",
    "        'Ridge Regression': Pipeline([\n",
    "            ('ridge', Ridge())\n",
    "        ]),\n",
    "        'Lasso Regression': Pipeline([\n",
    "            ('lasso', Lasso())\n",
    "        ]),\n",
    "        'Elastic Net Regression': Pipeline([\n",
    "            ('elasticnet', ElasticNet())\n",
    "        ]),\n",
    "        'LightGBM Regression': Pipeline([\n",
    "            ('lightgbm', LGBMRegressor())\n",
    "        ]),\n",
    "        'Random Forest Regression': Pipeline([\n",
    "            ('randomforest', RandomForestRegressor())\n",
    "        ]),\n",
    "        'XGBoost Regression': Pipeline([\n",
    "            ('xgboost', xgb.XGBRegressor(use_label_encoder=False, eval_metric='rmse'))\n",
    "        ]),\n",
    "    }\n",
    "\n",
    "    # Create GridSearchCV objects\n",
    "    models = {}\n",
    "    for name, pipeline in pipelines.items():\n",
    "        if name in param_grids:\n",
    "            models[name] = GridSearchCV(\n",
    "                estimator=pipeline,\n",
    "                param_grid=param_grids[name],\n",
    "                cv=5,\n",
    "                scoring='r2',\n",
    "                n_jobs=-1,\n",
    "                verbose=1\n",
    "            )\n",
    "        else:\n",
    "            models[name] = pipeline  # Linear Regression has no hyperparameters\n",
    "\n",
    "    # Training threshold\n",
    "    training_threshold = 7200  # seconds\n",
    "\n",
    "    # List of datasets\n",
    "    datasets = ['dataset1', 'dataset5']\n",
    "\n",
    "    # Results folder\n",
    "    results_dir = \"model_results\"\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "    # Models directory\n",
    "    models_dir = \"models\"\n",
    "    os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "    # Process each dataset\n",
    "    for dataset_name in datasets:\n",
    "        print(f\"\\nProcessing {dataset_name}\")\n",
    "\n",
    "        # Load pre-split data\n",
    "        data_path = f\"/home/dev/project/modelling/preprocessing/results/{dataset_name}\"\n",
    "        try:\n",
    "            train_data = pd.read_csv(os.path.join(data_path, \"train.csv\"))\n",
    "            test_data = pd.read_csv(os.path.join(data_path, \"test.csv\"))\n",
    "            val_data = pd.read_csv(os.path.join(data_path, \"val.csv\"))\n",
    "            print(f\"Successfully loaded pre-split data for {dataset_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading data for {dataset_name}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Define target variable\n",
    "        target_variable = 'diem_hp'\n",
    "        if target_variable not in train_data.columns:\n",
    "            print(f\"Target variable '{target_variable}' not found in training data for dataset '{dataset_name}'. Skipping this dataset.\")\n",
    "            continue\n",
    "\n",
    "        # Separate features and target\n",
    "        X_train = train_data.drop(columns=[target_variable])\n",
    "        y_train = train_data[target_variable]\n",
    "        X_test = test_data.drop(columns=[target_variable])\n",
    "        y_test = test_data[target_variable]\n",
    "        X_val = val_data.drop(columns=[target_variable])\n",
    "        y_val = val_data[target_variable]\n",
    "\n",
    "        # Handle missing values\n",
    "        X_train = X_train.fillna(0)\n",
    "        X_test = X_test.fillna(0)\n",
    "        X_val = X_val.fillna(0)\n",
    "        y_train = y_train.fillna(0)\n",
    "        y_test = y_test.fillna(0)\n",
    "        y_val = y_val.fillna(0)\n",
    "\n",
    "        print(\"All features are now numeric and missing values are handled.\")\n",
    "\n",
    "        # Prepare results storage\n",
    "        results = []\n",
    "\n",
    "        # Train and evaluate each model\n",
    "        for model_name, model in models.items():\n",
    "            print(f\"\\nTraining model: {model_name}\")\n",
    "            train_model(\n",
    "                model_name=model_name,\n",
    "                model=model,\n",
    "                X_train=X_train,\n",
    "                y_train=y_train,\n",
    "                X_test=X_test,\n",
    "                y_test=y_test,\n",
    "                results=results,\n",
    "                training_threshold=training_threshold,\n",
    "                dataset_name=dataset_name\n",
    "            )\n",
    "\n",
    "        # Save results to CSV\n",
    "        results_df = pd.DataFrame(results)\n",
    "        dataset_results_dir = os.path.join(results_dir, dataset_name)\n",
    "        os.makedirs(dataset_results_dir, exist_ok=True)\n",
    "        results_file = os.path.join(dataset_results_dir, 'model_results.csv')\n",
    "        results_df.to_csv(results_file, index=False)\n",
    "        print(f\"Results for {dataset_name} saved to {results_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
