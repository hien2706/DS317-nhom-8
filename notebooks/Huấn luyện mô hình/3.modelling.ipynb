{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import threading\n",
    "import time\n",
    "import pickle\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "from lightgbm import LGBMRegressor\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# Import BayesSearchCV from scikit-optimize\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer, Categorical  # Added import\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import (\n",
    "    Dense, LSTM, SimpleRNN, \n",
    "    Dropout\n",
    ")\n",
    "from keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import (\n",
    "    EarlyStopping, \n",
    "    ModelCheckpoint\n",
    ")\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "\n",
    "def train_model(model_name, model, X_train, y_train, X_test, y_test, results, training_threshold, dataset_name, is_optimized=False, X_val=None, y_val=None):\n",
    "    y_pred = [None]\n",
    "    training_time = [None]\n",
    "    optimization_time = [0]\n",
    "    training_completed = [False]\n",
    "\n",
    "    def train():\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            model_type = \"optimized\" if is_optimized else \"default\"\n",
    "            print(f\"Starting training for {model_name} ({model_type} parameters)...\")\n",
    "            \n",
    "            if isinstance(model, BayesSearchCV) and is_optimized:\n",
    "                opt_start = time.time()\n",
    "                model.fit(X_train, y_train)\n",
    "                optimization_time[0] = time.time() - opt_start\n",
    "            else:\n",
    "                if 'LightGBM' in model_name and not is_optimized:\n",
    "                    # For default parameters, get the LightGBM estimator directly from pipeline\n",
    "                    lgb_estimator = model.named_steps['lightgbm']\n",
    "                    if X_val is not None and y_val is not None:\n",
    "                        lgb_estimator.fit(\n",
    "                            X_train, y_train,\n",
    "                            eval_set=[(X_val, y_val)],\n",
    "                            early_stopping_rounds=20\n",
    "                        )\n",
    "                    else:\n",
    "                        lgb_estimator.fit(X_train, y_train)\n",
    "                else:\n",
    "                    model.fit(X_train, y_train)\n",
    "            \n",
    "            y_pred[0] = model.predict(X_test)\n",
    "            training_time[0] = time.time() - start_time\n",
    "            training_completed[0] = True\n",
    "            print(f\"Completed training for {model_name} in {training_time[0]:.2f} seconds.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error training model {model_name}: {e}\")\n",
    "            training_completed[0] = False\n",
    "\n",
    "    thread = threading.Thread(target=train)\n",
    "    thread.start()\n",
    "    thread.join(timeout=training_threshold)\n",
    "\n",
    "    if not training_completed[0]:\n",
    "        print(f\"Model {model_name} exceeded training time ({training_threshold} seconds) or encountered an error.\")\n",
    "        y_pred[0] = np.nan\n",
    "        training_time[0] = np.nan\n",
    "    else:\n",
    "        mse = mean_squared_error(y_test, y_pred[0])\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(y_test, y_pred[0])\n",
    "        r_squared = r2_score(y_test, y_pred[0])\n",
    "\n",
    "        n = len(y_test)\n",
    "        p = X_test.shape[1]\n",
    "        if n > p + 1 and p > 0:\n",
    "            adjusted_r_squared = 1 - (1 - r_squared) * ((n - 1) / (n - p - 1))\n",
    "        else:\n",
    "            adjusted_r_squared = r_squared\n",
    "\n",
    "        print(f\"Model {model_name} trained successfully in {training_time[0]:.2f} seconds.\")\n",
    "        print(f\"MSE: {mse}, RMSE: {rmse}, MAE: {mae}, R²: {r_squared}, Adjusted R²: {adjusted_r_squared}\")\n",
    "\n",
    "        result = {\n",
    "            'Model': f\"{model_name} ({'Optimized' if is_optimized else 'Default'})\",\n",
    "            'Dataset': dataset_name,\n",
    "            'Parameter Type': 'Optimized' if is_optimized else 'Default',\n",
    "            'Training Time (s)': training_time[0],\n",
    "            'Optimization Time (s)': optimization_time[0],\n",
    "            'MSE': mse,\n",
    "            'RMSE': rmse,\n",
    "            'MAE': mae,\n",
    "            'R2 Score': r_squared,\n",
    "            'Adjusted R2 Score': adjusted_r_squared\n",
    "        }\n",
    "\n",
    "        if isinstance(model, BayesSearchCV) and is_optimized:\n",
    "            result['Best Params'] = str(model.best_params_)\n",
    "        elif not is_optimized:\n",
    "            result['Parameters'] = str(model.get_params())\n",
    "\n",
    "        results.append(result)\n",
    "\n",
    "def main():\n",
    "    # Define default parameters for each model\n",
    "    default_params = {\n",
    "        'Ridge Regression': {\n",
    "            'ridge__alpha': 1.0\n",
    "        },\n",
    "        'Lasso Regression': {\n",
    "            'lasso__alpha': 0.1\n",
    "        },\n",
    "        'Elastic Net Regression': {\n",
    "            'elasticnet__alpha': 0.1,\n",
    "            'elasticnet__l1_ratio': 0.5\n",
    "        },\n",
    "        'Random Forest Regression': {\n",
    "            'randomforest__n_estimators': 100,\n",
    "            'randomforest__max_depth': 10,\n",
    "            'randomforest__min_samples_split': 2\n",
    "        },\n",
    "        'XGBoost Regression': {\n",
    "            'xgboost__learning_rate': 0.01,\n",
    "            'xgboost__max_depth': 3,\n",
    "            'xgboost__n_estimators': 100\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Update parameter search spaces using skopt.space dimensions\n",
    "    param_spaces = {\n",
    "        'Ridge Regression': {\n",
    "            'ridge__alpha': Real(0.1, 5.0, prior='log-uniform')\n",
    "        },\n",
    "        'Lasso Regression': {\n",
    "            'lasso__alpha': Real(0.01, 0.5, prior='log-uniform')\n",
    "        },\n",
    "        'Elastic Net Regression': {\n",
    "            'elasticnet__alpha': Real(0.01, 0.5, prior='log-uniform'),\n",
    "            'elasticnet__l1_ratio': Real(0.2, 0.8)\n",
    "        },\n",
    "        'LightGBM Regression': {\n",
    "            'lightgbm__num_leaves': Integer(20, 40),\n",
    "            'lightgbm__learning_rate': Real(0.01, 0.03, prior='log-uniform'),\n",
    "            'lightgbm__n_estimators': Integer(50, 150)\n",
    "        },\n",
    "        'Random Forest Regression': {\n",
    "            'randomforest__n_estimators': Integer(50, 100),\n",
    "            'randomforest__max_depth': Categorical([5, 10]),\n",
    "            'randomforest__min_samples_split': Integer(2, 4)\n",
    "        },\n",
    "        'XGBoost Regression': {\n",
    "            'xgboost__learning_rate': Real(0.01, 0.03, prior='log-uniform'),\n",
    "            'xgboost__max_depth': Integer(3, 5),\n",
    "            'xgboost__n_estimators': Integer(50, 150)\n",
    "        }\n",
    "    }\n",
    "    pipelines = {\n",
    "        'Linear Regression': Pipeline([\n",
    "            ('linearregression', LinearRegression())\n",
    "        ]),\n",
    "        'Ridge Regression': Pipeline([\n",
    "            ('ridge', Ridge())\n",
    "        ]),\n",
    "        'Lasso Regression': Pipeline([\n",
    "            ('lasso', Lasso())\n",
    "        ]),\n",
    "        'Elastic Net Regression': Pipeline([\n",
    "            ('elasticnet', ElasticNet())\n",
    "        ]),\n",
    "        'LightGBM Regression': Pipeline([\n",
    "            ('lightgbm', LGBMRegressor(\n",
    "                num_leaves=31,\n",
    "                learning_rate=0.01,\n",
    "                n_estimators=100\n",
    "            ))\n",
    "        ]),\n",
    "        'Random Forest Regression': Pipeline([\n",
    "            ('randomforest', RandomForestRegressor())\n",
    "        ]),\n",
    "        'XGBoost Regression': Pipeline([\n",
    "            ('xgboost', xgb.XGBRegressor(use_label_encoder=False, eval_metric='rmse'))\n",
    "        ]),\n",
    "    }\n",
    "\n",
    "    # Remove LightGBM from default_params since we're setting them directly in the pipeline\n",
    "    default_params = {\n",
    "        'Ridge Regression': {\n",
    "            'ridge__alpha': 1.0\n",
    "        },\n",
    "        'Lasso Regression': {\n",
    "            'lasso__alpha': 0.1\n",
    "        },\n",
    "        'Elastic Net Regression': {\n",
    "            'elasticnet__alpha': 0.1,\n",
    "            'elasticnet__l1_ratio': 0.5\n",
    "        },\n",
    "        'Random Forest Regression': {\n",
    "            'randomforest__n_estimators': 100,\n",
    "            'randomforest__max_depth': 10,\n",
    "            'randomforest__min_samples_split': 2\n",
    "        },\n",
    "        'XGBoost Regression': {\n",
    "            'xgboost__learning_rate': 0.01,\n",
    "            'xgboost__max_depth': 3,\n",
    "            'xgboost__n_estimators': 100\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Set default parameters for each pipeline\n",
    "    for name, pipeline in pipelines.items():\n",
    "        if name in default_params:\n",
    "            pipeline.set_params(**default_params[name])\n",
    "\n",
    "    models = {}\n",
    "    for name, pipeline in pipelines.items():\n",
    "        if name in param_spaces:\n",
    "            models[name] = BayesSearchCV(\n",
    "                estimator=pipeline,\n",
    "                search_spaces=param_spaces[name],\n",
    "                cv=3,\n",
    "                scoring='r2',\n",
    "                n_jobs=1,\n",
    "                verbose=1,\n",
    "                n_iter=5,  # Reduced number of iterations\n",
    "                random_state=42\n",
    "            )\n",
    "        else:\n",
    "            models[name] = pipeline\n",
    "\n",
    "    # Training threshold\n",
    "    training_threshold = 7200  # seconds\n",
    "\n",
    "    # List of datasets\n",
    "    datasets = ['dataset1', 'dataset5']\n",
    "\n",
    "    # Results folder\n",
    "    results_dir = \"model_results\"\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "    # Models directory\n",
    "    models_dir = \"models\"\n",
    "    os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "    # Process each dataset\n",
    "    for dataset_name in datasets:\n",
    "        print(f\"\\nProcessing {dataset_name}\")\n",
    "\n",
    "        # Load pre-split data\n",
    "        data_path = f\"/home/dev/project/modelling/preprocessing/results/{dataset_name}\"\n",
    "        try:\n",
    "            train_data = pd.read_csv(os.path.join(data_path, \"train.csv\"))\n",
    "            test_data = pd.read_csv(os.path.join(data_path, \"test.csv\"))\n",
    "            val_data = pd.read_csv(os.path.join(data_path, \"val.csv\"))\n",
    "            print(f\"Successfully loaded pre-split data for {dataset_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading data for {dataset_name}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # drop column 'mssv' if it exists\n",
    "        if 'mssv' in train_data.columns:\n",
    "            train_data = train_data.drop(columns=['mssv'])\n",
    "            test_data = test_data.drop(columns=['mssv'])\n",
    "            val_data = val_data.drop(columns=['mssv'])\n",
    "        \n",
    "        # Define target variable\n",
    "        target_variable = 'diem_hp'\n",
    "        if target_variable not in train_data.columns:\n",
    "            print(f\"Target variable '{target_variable}' not found in training data for dataset '{dataset_name}'. Skipping this dataset.\")\n",
    "            continue\n",
    "\n",
    "        # Separate features and target\n",
    "        X_train = train_data.drop(columns=[target_variable])\n",
    "        y_train = train_data[target_variable]\n",
    "        X_test = test_data.drop(columns=[target_variable])\n",
    "        y_test = test_data[target_variable]\n",
    "        X_val = val_data.drop(columns=[target_variable])\n",
    "        y_val = val_data[target_variable]\n",
    "\n",
    "        # Handle missing values\n",
    "        X_train = X_train.fillna(0)\n",
    "        X_test = X_test.fillna(0)\n",
    "        X_val = X_val.fillna(0)\n",
    "        y_train = y_train.fillna(0)\n",
    "        y_test = y_test.fillna(0)\n",
    "        y_val = y_val.fillna(0)\n",
    "\n",
    "        print(\"All features are now numeric and missing values are handled.\")\n",
    "\n",
    "        # Prepare results storage\n",
    "        results = []\n",
    "\n",
    "        # Train and evaluate each model with default parameters first\n",
    "        for model_name, pipeline in pipelines.items():\n",
    "            if model_name in default_params:\n",
    "                default_model = pipeline.set_params(**default_params[model_name])\n",
    "                print(f\"\\nTraining model: {model_name} (default parameters)\")\n",
    "                train_model(\n",
    "                    model_name=model_name,\n",
    "                    model=default_model,\n",
    "                    X_train=X_train,\n",
    "                    y_train=y_train,\n",
    "                    X_test=X_test,\n",
    "                    y_test=y_test,\n",
    "                    results=results,\n",
    "                    training_threshold=training_threshold,\n",
    "                    dataset_name=dataset_name,\n",
    "                    is_optimized=False,\n",
    "                    X_val=X_val,\n",
    "                    y_val=y_val\n",
    "                )\n",
    "\n",
    "        # Train and evaluate each model with optimization\n",
    "        for model_name, model in models.items():\n",
    "            if model_name in param_spaces:\n",
    "                print(f\"\\nTraining model: {model_name} (with optimization)\")\n",
    "                train_model(\n",
    "                    model_name=model_name,\n",
    "                    model=model,\n",
    "                    X_train=X_train,\n",
    "                    y_train=y_train,\n",
    "                    X_test=X_test,\n",
    "                    y_test=y_test,\n",
    "                    results=results,\n",
    "                    training_threshold=training_threshold,\n",
    "                    dataset_name=dataset_name,\n",
    "                    is_optimized=True,\n",
    "                    X_val=X_val,\n",
    "                    y_val=y_val\n",
    "                )\n",
    "\n",
    "        # Save results to CSV with both default and optimized results\n",
    "        results_df = pd.DataFrame(results)\n",
    "        dataset_results_dir = os.path.join(results_dir, dataset_name)\n",
    "        os.makedirs(dataset_results_dir, exist_ok=True)\n",
    "        results_file = os.path.join(dataset_results_dir, 'model_results_comparison.csv')\n",
    "        results_df.to_csv(results_file, index=False)\n",
    "        print(f\"Results for {dataset_name} saved to {results_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN + LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class earlyStoppingWithMinEpochs(keras.callbacks.EarlyStopping):\n",
    "    def __init__(self, monitor='val_loss',\n",
    "             min_delta=0, patience=10, verbose=0, mode='auto', min_epoch = 100): # add argument for starting epoch\n",
    "        super(earlyStoppingWithMinEpochs, self).__init__()\n",
    "        self.min_epoch = min_epoch\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if epoch > self.min_epoch:\n",
    "            super().on_epoch_end(epoch, logs)\n",
    "            \n",
    "def build_lstm(input_shape):\n",
    "    model = Sequential([\n",
    "        tf.keras.Input(shape=input_shape),\n",
    "        LSTM(128, \n",
    "             return_sequences=False, \n",
    "             kernel_regularizer=l2(0.001), \n",
    "             recurrent_regularizer=l2(0.001)),\n",
    "        Dropout(0.1),\n",
    "        Dense(1, kernel_regularizer=l2(0.001))\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def build_rnn_model(input_shape):\n",
    "    model = Sequential([\n",
    "        tf.keras.Input(shape=input_shape),\n",
    "        SimpleRNN(128, \n",
    "                  return_sequences=False),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def compile_and_train_model(model, X_train, y_train, X_val, y_val, model_name, early_stopping_params, checkpoint_params, epochs=500, batch_size=1024):\n",
    "    early_stopping_callback = EarlyStopping(**early_stopping_params)\n",
    "    checkpoint_callback = ModelCheckpoint(model_name, **checkpoint_params)\n",
    "    \n",
    "    model.compile(\n",
    "        loss='mean_squared_error', \n",
    "        optimizer='adam', \n",
    "        metrics=['r2_score', 'root_mean_squared_error', 'mean_absolute_error']\n",
    "    )\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train, y_train, \n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=epochs, \n",
    "        batch_size=batch_size, \n",
    "        callbacks=[early_stopping_callback, checkpoint_callback],\n",
    "        verbose=1\n",
    "    )\n",
    "    return history\n",
    "\n",
    "def eval_dl(model, X_train, y_train, X_val, y_val, X_test, y_test, dataset_name, model_name):\n",
    "    res_df = pd.DataFrame(columns=['Dataset', 'model', 'SET', 'MSE', 'RMSE', 'R2', 'ADJ_R2', 'MAE', 'MAPE'])\n",
    "\n",
    "    # Predictions\n",
    "    train_pred = model.predict(X_train)\n",
    "    val_pred = model.predict(X_val)\n",
    "    test_pred = model.predict(X_test)\n",
    "\n",
    "    # TRAIN\n",
    "    train_mse = mean_squared_error(y_train, train_pred)\n",
    "    train_rmse = np.sqrt(train_mse)\n",
    "    train_r2 = r2_score(y_train, train_pred)\n",
    "    train_mae = mean_absolute_error(y_train, train_pred)\n",
    "    train_adj_r2 = 1 - (1-train_r2) * (len(y_train)-1)/(len(y_train)-X_train.shape[1]-1)\n",
    "    train_mape = mean_absolute_percentage_error(y_train, train_pred)\n",
    "\n",
    "    # VAL\n",
    "    val_mse = mean_squared_error(y_val, val_pred)\n",
    "    val_rmse = np.sqrt(val_mse)\n",
    "    val_r2 = r2_score(y_val, val_pred)\n",
    "    val_mae = mean_absolute_error(y_val, val_pred)\n",
    "    val_adj_r2 = 1 - (1-val_r2) * (len(y_val)-1)/(len(y_val)-X_val.shape[1]-1)\n",
    "    val_mape = mean_absolute_percentage_error(y_val, val_pred)\n",
    "\n",
    "    # TEST\n",
    "    test_mse = mean_squared_error(y_test, test_pred)\n",
    "    test_rmse = np.sqrt(test_mse)\n",
    "    test_r2 = r2_score(y_test, test_pred)\n",
    "    test_mae = mean_absolute_error(y_test, test_pred)\n",
    "    test_adj_r2 = 1 - (1-test_r2) * (len(y_test)-1)/(len(y_test)-X_test.shape[1]-1)\n",
    "    test_mape = mean_absolute_percentage_error(y_test, test_pred)\n",
    "\n",
    "    results = pd.DataFrame([\n",
    "        {'Dataset': dataset_name, 'model': model_name, 'SET': 'train', 'MSE': train_mse, \n",
    "         'RMSE': train_rmse, 'R2': train_r2, 'MAPE': train_mape,\n",
    "         'ADJ_R2': train_adj_r2, 'MAE': train_mae},\n",
    "        {'Dataset': dataset_name, 'model': model_name, 'SET': 'val', 'MSE': val_mse, \n",
    "         'RMSE': val_rmse, 'R2': val_r2, 'MAPE': val_mape,\n",
    "         'ADJ_R2': val_adj_r2, 'MAE': val_mae},\n",
    "        {'Dataset': dataset_name, 'model': model_name, 'SET': 'test', 'MSE': test_mse, \n",
    "         'RMSE': test_rmse, 'R2': test_r2, 'MAPE': test_mape,\n",
    "         'ADJ_R2': test_adj_r2, 'MAE': test_mae}\n",
    "    ])\n",
    "    res_df = pd.concat([res_df, results], ignore_index=True)\n",
    "    return res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    datasets = ['dataset1','dataset5']\n",
    "    results_dir = \"model_results\"\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    models_dir = \"models\"\n",
    "    os.makedirs(models_dir, exist_ok=True)\n",
    "    # res = pd.DataFrame(columns=['Dataset', 'model', 'SET', 'MSE', 'RMSE', 'R2', 'ADJ_R2', 'MAE', 'MAPE'])\n",
    "    \n",
    "    \n",
    "    for dataset_name in datasets:\n",
    "        print(f\"\\nProcessing {dataset_name}\")\n",
    "\n",
    "        # Load pre-split data\n",
    "        data_path = f\"/home/dev/project/modelling/preprocessing/results/{dataset_name}\"\n",
    "        try:\n",
    "            train_data = pd.read_csv(os.path.join(data_path, \"train.csv\"))\n",
    "            test_data = pd.read_csv(os.path.join(data_path, \"test.csv\"))\n",
    "            val_data = pd.read_csv(os.path.join(data_path, \"val.csv\"))\n",
    "            print(f\"Successfully loaded pre-split data for {dataset_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading data for {dataset_name}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # drop column 'mssv' if it exists\n",
    "        if 'mssv' in train_data.columns:\n",
    "            train_data = train_data.drop(columns=['mssv'])\n",
    "            test_data = test_data.drop(columns=['mssv'])\n",
    "            val_data = val_data.drop(columns=['mssv'])\n",
    "        \n",
    "        # Define target variable\n",
    "        target_variable = 'diem_hp'\n",
    "        if target_variable not in train_data.columns:\n",
    "            print(f\"Target variable '{target_variable}' not found in training data for dataset '{dataset_name}'. Skipping this dataset.\")\n",
    "            continue\n",
    "\n",
    "        # Separate features and target\n",
    "        X_train = train_data.drop(columns=[target_variable])\n",
    "        y_train = train_data[target_variable]\n",
    "        X_test = test_data.drop(columns=[target_variable])\n",
    "        y_test = test_data[target_variable]\n",
    "        X_val = val_data.drop(columns=[target_variable])\n",
    "        y_val = val_data[target_variable]\n",
    "\n",
    "        # Handle missing values\n",
    "        X_train = X_train.fillna(0)\n",
    "        X_test = X_test.fillna(0)\n",
    "        X_val = X_val.fillna(0)\n",
    "        y_train = y_train.fillna(0)\n",
    "        y_test = y_test.fillna(0)\n",
    "        y_val = y_val.fillna(0)\n",
    "\n",
    "        print(\"All features are now numeric and missing values are handled.\")\n",
    "        \n",
    "        res = []\n",
    "        lstm_model = build_lstm((\n",
    "            X_train.shape[1],1\n",
    "        ))\n",
    "        early_stopping_params_lstm = {'monitor': 'val_loss', 'patience': 10, 'min_epoch': 100}\n",
    "        checkpoint_params_lstm = {'monitor': 'val_loss', 'verbose': 1, 'save_best_only': True, 'mode': 'min'}\n",
    "        lstm_history = compile_and_train_model(lstm_model, X_train, y_train, X_val, y_val, \n",
    "                                               os.path.join(models_dir, f'lstm_{dataset_name}.keras'), \n",
    "                                               early_stopping_params_lstm, checkpoint_params_lstm)\n",
    "        lstm_results = eval_dl(lstm_model, X_train, y_train, X_val, y_val, X_test, y_test, dataset_name, 'lstm')\n",
    "        res.append(lstm_results)\n",
    "        \n",
    "        rnn_model = build_rnn((\n",
    "            X_train.shape[1], 1))\n",
    "        early_stopping_params_rnn = {'monitor': 'val_loss', 'patience': 10, 'min_epoch': 100}\n",
    "        checkpoint_params_rnn = {'monitor': 'val_loss', 'verbose': 1, 'save_best_only': True, 'mode': 'min'}\n",
    "        rnn_history = compile_and_train_model(rnn_model, X_train, y_train, X_val, y_val, \n",
    "                                              os.path.join(models_dir, f'rnn_{dataset_name}.keras'), \n",
    "                                              early_stopping_params_rnn, checkpoint_params_rnn\n",
    "        rnn_results = eval_dl(rnn_model, X_train, y_train, X_val, y_val, X_test, y_test, dataset_name, 'rnn')\n",
    "        res.append(rnn_results)\n",
    "\n",
    "        final_results = pd.concat(res, ignore_index=True)\n",
    "        final_results.to_csv(os.path.join(results_dir, f'results_{dataset_name}.csv'), index=False)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TabNet + FT-Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U pytorch_tabular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_tabular import TabularModel \n",
    "from pytorch_tabular.models import (\n",
    "    CategoryEmbeddingModelConfig, \n",
    "    TabNetModelConfig, \n",
    "    TabTransformerConfig,\n",
    "    FTTransformerConfig\n",
    ")\n",
    "from pytorch_tabular.config import (\n",
    "    DataConfig,\n",
    "    OptimizerConfig,\n",
    "    TrainerConfig,\n",
    "    ExperimentConfig,\n",
    ")\n",
    "from pytorch_tabular import available_models\n",
    "available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tabnet_model(data_config, optimizer_config, trainer_config):\n",
    "    tabnet_config = TabNetModelConfig(\n",
    "        task=\"regression\",\n",
    "        n_d=8,\n",
    "        n_a=8,\n",
    "        n_steps=3,\n",
    "        n_independent=2,\n",
    "        n_shared=2,\n",
    "        virtual_batch_size=128,\n",
    "        learning_rate=1e-3,\n",
    "        target_range=[(0, 10)],\n",
    "        loss=\"MSELoss\",\n",
    "        seed=42,\n",
    "    )\n",
    "    tabnet_model = TabularModel(\n",
    "        data_config=data_config,\n",
    "        model_config=tabnet_config,\n",
    "        optimizer_config=optimizer_config,\n",
    "        trainer_config=trainer_config,\n",
    "        verbose=True\n",
    "    )\n",
    "    return tabnet_model\n",
    "\n",
    "def build_ftt_model(data_config, optimizer_config, trainer_config):\n",
    "    ftt_config = FTTransformerConfig(\n",
    "        task=\"regression\",\n",
    "        input_embed_dim=32,\n",
    "        embedding_bias=True,\n",
    "        attn_feature_importance=True,\n",
    "        num_heads=8,\n",
    "        num_attn_blocks=6,\n",
    "        attn_dropout=0.1,\n",
    "        add_norm_dropout=0.1,\n",
    "        ff_dropout=0.1,\n",
    "        ff_hidden_multiplier=4,\n",
    "        transformer_activation=\"GEGLU\",\n",
    "        seed=42,\n",
    "        learning_rate=1e-3,\n",
    "        target_range=[(0, 10)],\n",
    "        loss=\"MSELoss\",\n",
    "    )\n",
    "    ftt_model = TabularModel(\n",
    "        data_config=data_config,\n",
    "        model_config=ftt_config,\n",
    "        optimizer_config=optimizer_config,\n",
    "        trainer_config=trainer_config,\n",
    "        verbose=True\n",
    "    )\n",
    "    return ftt_model\n",
    "\n",
    "def train_and_evaluate_model(model, train_data, val_data, test_data, model_path):\n",
    "    model.fit(train=train_data, validation=val_data)\n",
    "    result = model.evaluate(test_data)\n",
    "    predictions = model.predict(test_data)\n",
    "    model.save_model(model_path)\n",
    "    return result, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    datasets = ['dataset1','dataset5']\n",
    "    results_dir = \"model_results\"\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    models_dir = \"models\"\n",
    "    os.makedirs(models_dir, exist_ok=True)\n",
    "    # res = pd.DataFrame(columns=['Dataset', 'model', 'SET', 'MSE', 'RMSE', 'R2', 'ADJ_R2', 'MAE', 'MAPE'])\n",
    "    \n",
    "    \n",
    "    for dataset_name in datasets:\n",
    "        print(f\"\\nProcessing {dataset_name}\")\n",
    "\n",
    "        # Load pre-split data\n",
    "        data_path = f\"/home/dev/project/modelling/preprocessing/results/{dataset_name}\"\n",
    "        try:\n",
    "            train_data = pd.read_csv(os.path.join(data_path, \"train.csv\"))\n",
    "            test_data = pd.read_csv(os.path.join(data_path, \"test.csv\"))\n",
    "            val_data = pd.read_csv(os.path.join(data_path, \"val.csv\"))\n",
    "            print(f\"Successfully loaded pre-split data for {dataset_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading data for {dataset_name}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # drop column 'mssv' if it exists\n",
    "        if 'mssv' in train_data.columns:\n",
    "            train_data = train_data.drop(columns=['mssv'])\n",
    "            test_data = test_data.drop(columns=['mssv'])\n",
    "            val_data = val_data.drop(columns=['mssv'])\n",
    "        \n",
    "        # Define target variable\n",
    "        target_variable = ['diem_hp']\n",
    "        dtbhk_seq_cols = []\n",
    "        sotchk_seq_cols = []\n",
    "        for i in range(1, 23):\n",
    "            dtbhk_seq_cols.append(f'dtbhk{i}')\n",
    "            sotchk_seq_cols.append(f'sotchk{i}')\n",
    "        seq_features = dtbhk_seq_cols + sotchk_seq_cols\n",
    "        features = train_data.columns.drop(target_variable) #all\n",
    "        con_ext = ['sotc']\n",
    "        con_cols = seq_features + con_ext\n",
    "        cat_cols = [col for col in features if col not in con_cols]\n",
    "        \n",
    "        if target_variable[0] not in train_data.columns:\n",
    "            print(f\"Target variable '{target_variable}' not found in training data for dataset '{dataset_name}'. Skipping this dataset.\")\n",
    "            continue\n",
    "            \n",
    "        data_config = DataConfig(\n",
    "            target = target_variable,\n",
    "            continuous_cols=list(con_cols),\n",
    "            categorical_cols=list(cat_cols),\n",
    "        )\n",
    "        trainer_config = TrainerConfig(\n",
    "            auto_lr_find=True,  \n",
    "            batch_size=1024,\n",
    "            min_epochs=100,\n",
    "            max_epochs=500,\n",
    "            accelerator=\"gpu\",\n",
    "            early_stopping_patience=10,\n",
    "        )\n",
    "        \n",
    "        # exp_config = ExperimentConfig(\n",
    "        #     project_name=EXP_PROJECT_NAME,\n",
    "        #     run_name=\"ftt-df5-all-update\",\n",
    "        #     exp_watch=\"gradients\",\n",
    "        #     log_target=\"wandb\",\n",
    "        #     log_logits=False,\n",
    "        # )\n",
    "        \n",
    "        optimizer_config = OptimizerConfig()\n",
    "        # Separate features and target\n",
    "        X_train = train_data.drop(columns=target_variable)\n",
    "        y_train = train_data[target_variable]\n",
    "        X_test = test_data.drop(columns=target_variable)\n",
    "        y_test = test_data[target_variable]\n",
    "        X_val = val_data.drop(columns=target_variable)\n",
    "        y_val = val_data[target_variable]\n",
    "\n",
    "        # Handle missing values\n",
    "        X_train = X_train.fillna(0)\n",
    "        X_test = X_test.fillna(0)\n",
    "        X_val = X_val.fillna(0)\n",
    "        y_train = y_train.fillna(0)\n",
    "        y_test = y_test.fillna(0)\n",
    "        y_val = y_val.fillna(0)\n",
    "        print(\"All features are now numeric and missing values are handled.\")\n",
    "        \n",
    "        res = []\n",
    "\n",
    "        tabnet_model = build_tabnet_model(data_config, optimizer_config, trainer_config)\n",
    "        tabnet_res_default_default, tabnet_predictions = train_and_evaluate_model(tabnet_model, \n",
    "                                                                     train_data, val_data, test_data, \n",
    "                                                                     os.path.join(models_dir, f'tabnet_{dataset_name}.pt'))\n",
    "        tabnet_res = eval_dl(tabnet_model, X_train, y_train, X_val, y_val, X_test, y_test, dataset_name, 'tabnet')\n",
    "        res.append(tabnet_res)\n",
    "        \n",
    "        ftt_model = build_ftt_model(data_config, optimizer_config, trainer_config)\n",
    "        ftt_res_default, ftt_predictions = train_and_evaluate_model(ftt_model, \n",
    "                                                               train_data, val_data, test_data, \n",
    "                                                               os.path.join(models_dir, f'ftt_{dataset_name}.pt'))\n",
    "        ftt_res = eval_dl(ftt_model, X_train, y_train, X_val, y_val, X_test, y_test, dataset_name, 'ftt')\n",
    "        res.append(ftt_res)\n",
    "        \n",
    "        final_results = pd.concat(res, ignore_index=True)\n",
    "        final_results.to_csv(os.path.join(results_dir, f'results_{dataset_name}.csv'), index=False)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
